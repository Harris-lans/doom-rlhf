{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF Trainining Pipeline for PPO Agent to Play Levels from the Doom Game\n",
    "\n",
    "## Creating UI Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from typing import Callable\n",
    "from datetime import datetime\n",
    "\n",
    "# Function for rendering widgets\n",
    "def create_html_heading(text, level=1, centered=True):\n",
    "    html_tag = f'<h{level}>{text}</h{level}>'\n",
    "    layout = widgets.Layout(display='flex', justify_content='center') if centered else None\n",
    "    return widgets.HTML(html_tag, layout=layout)\n",
    "\n",
    "def create_video_player(video_path: str):\n",
    "    return widgets.Video.from_file(video_path)\n",
    "\n",
    "def create_button(description, tooltip):\n",
    "    return widgets.Button(description=description, disabled=False, button_style='',\n",
    "                          tooltip=tooltip, layout=widgets.Layout(display='flex', justify_content='center', margin='4px'))\n",
    "\n",
    "def create_loading_spinner():\n",
    "    css = \"\"\"\n",
    "    .loader {\n",
    "        border: 8px solid #f3f3f3;\n",
    "        border-top: 8px solid #3498db;\n",
    "        border-radius: 50%;\n",
    "        width: 100px;\n",
    "        height: 100px;\n",
    "        animation: spin 2s linear infinite;\n",
    "    }\n",
    "\n",
    "    @keyframes spin {\n",
    "        0% { transform: rotate(0deg); }\n",
    "        100% { transform: rotate(360deg); }\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Create the custom CSS and apply it to the heading\n",
    "    heading_style = widgets.HTML(f'<style>{css}</style>')\n",
    "    # Create the loading spinner widget\n",
    "    loading_spinner = widgets.HTML('<div class=\"loader\"></div>', layout=widgets.Layout(display='flex', justify_content='center'))\n",
    "    \n",
    "    return heading_style, loading_spinner\n",
    "\n",
    "def create_preference_selection_component(trajectory_1_video_path: str, trajectory_2_video_path: str, on_trajectory_1_chosen: Callable, on_trajectory_2_chosen: Callable, on_both_trajectories_chosen: Callable):\n",
    "    # Creating GUI Components\n",
    "    window_label = create_html_heading('Which trajectory do you prefer?', centered=True)\n",
    "    trajectory_1_label = create_html_heading('Trajectory 1', level=2, centered=True)\n",
    "    trajectory_2_label = create_html_heading('Trajectory 2', level=2, centered=True)\n",
    "    trajectory_1_video_player = create_video_player(trajectory_1_video_path)\n",
    "    trajectory_2_video_player = create_video_player(trajectory_2_video_path)\n",
    "    prefer_trajectory_1_button = create_button('Select 1', 'You prefer Trajectory 1')\n",
    "    prefer_trajectory_2_button = create_button('Select 2', 'You prefer Trajectory 2')\n",
    "    prefer_both_trajectories_button = create_button('Both', 'You prefer both Trajectories')\n",
    "    \n",
    "    prefer_trajectory_1_button.on_click(on_trajectory_1_chosen)\n",
    "    prefer_trajectory_2_button.on_click(on_trajectory_2_chosen)\n",
    "    prefer_both_trajectories_button.on_click(on_both_trajectories_chosen)\n",
    "\n",
    "    window_label.layout.flex = '1'\n",
    "\n",
    "    # Rendering components\n",
    "    preference_selection_layout = widgets.VBox([\n",
    "        window_label,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                trajectory_1_label,\n",
    "                trajectory_1_video_player,\n",
    "                prefer_trajectory_1_button\n",
    "            ], layout=widgets.Layout(align_items='center', justify_content='space-between', width='40%', height='100%')),\n",
    "            widgets.VBox([\n",
    "                trajectory_2_label,\n",
    "                trajectory_2_video_player,\n",
    "                prefer_trajectory_2_button\n",
    "            ], layout=widgets.Layout(align_items='center', justify_content='space-between', width='40%', height='100%'))\n",
    "        ], layout=widgets.Layout(justify_content='space-between', flex='4', width='100%')),\n",
    "        widgets.HBox([\n",
    "            prefer_both_trajectories_button\n",
    "        ], layout=widgets.Layout(justify_content='space-between', flex='1'))\n",
    "    ], layout=widgets.Layout(align_items='center', justify_content='space-between'))\n",
    "\n",
    "    return preference_selection_layout\n",
    "\n",
    "def create_loading_component(message):\n",
    "    # Create the heading widget and apply the style\n",
    "    heading = create_html_heading(message, centered=True)\n",
    "    # Create the loading spinner widget\n",
    "    heading_style, loading_spinner = create_loading_spinner()\n",
    "\n",
    "    # Create the outer VBox layout with widgets\n",
    "    outer_vbox = widgets.VBox([heading_style, heading, loading_spinner], layout=widgets.Layout(align_items='center', justify_content='center'))\n",
    "\n",
    "    return outer_vbox\n",
    "\n",
    "def create_logs_component():\n",
    "    logs_label = widgets.HTML('<h1>Logs</h1>')\n",
    "    logs_output = widgets.Output(description=\"Output\")\n",
    "    logs_layout = widgets.VBox([\n",
    "            logs_label,\n",
    "            widgets.Box([\n",
    "                logs_output\n",
    "            ], layout=widgets.Layout(flex='1', width='98%', overflow_y='scroll'))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return logs_layout, logs_output\n",
    "\n",
    "logs_component, logs_output = create_logs_component()\n",
    "layout = widgets.HBox([])\n",
    "\n",
    "def log_info(message: str):\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    message = f\"[{current_timestamp}][INFO] {message}\"\n",
    "    print(message)\n",
    "\n",
    "def log_error(message: str):\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    message = f\"[{current_timestamp}][ERROR] {message}\\n\"\n",
    "    logs_output.append_stderr(message)\n",
    "\n",
    "def hide_all_screens():\n",
    "    layout.children = []\n",
    "\n",
    "def show_preference_selection_screen(trajectory_1_video_path: str, trajectory_2_video_path: str, on_trajectory_1_chosen: Callable, on_trajectory_2_chosen: Callable, on_both_trajectories_chosen: Callable):\n",
    "    preference_selection_component = create_preference_selection_component(trajectory_1_video_path, trajectory_2_video_path, on_trajectory_1_chosen, on_trajectory_2_chosen, on_both_trajectories_chosen)\n",
    "\n",
    "    # Updating layout of components to fit the screen layout\n",
    "    preference_selection_component.layout.padding = '15px' \n",
    "    preference_selection_component.layout.margin = '7.5px' \n",
    "    preference_selection_component.layout.border = '3px dashed cornflowerblue' \n",
    "    preference_selection_component.layout.flex = '1' \n",
    "    preference_selection_component.layout.height = '600px'\n",
    "    \n",
    "    logs_component.layout.padding = '15px'\n",
    "    logs_component.layout.margin = '7.5px' \n",
    "    logs_component.layout.border = '3px dashed cornflowerblue' \n",
    "    logs_component.layout.flex = '1' \n",
    "    logs_component.layout.height = '600px'\n",
    "\n",
    "    layout.children = [ preference_selection_component, logs_component ]\n",
    "\n",
    "def show_loading_screen(message: str):\n",
    "    loading_component = create_loading_component(message)\n",
    "\n",
    "    # Updating layout of components to fit the screen layout\n",
    "    loading_component.layout.padding = '15px'\n",
    "    loading_component.layout.margin = '7.5px' \n",
    "    loading_component.layout.border = '3px dashed cornflowerblue' \n",
    "    loading_component.layout.flex = '1' \n",
    "    loading_component.layout.height = '600px'\n",
    "    \n",
    "    logs_component.layout.padding = '15px'\n",
    "    logs_component.layout.margin = '7.5px' \n",
    "    logs_component.layout.border = '3px dashed cornflowerblue' \n",
    "    logs_component.layout.flex = '1' \n",
    "    logs_component.layout.height = '600px'\n",
    "\n",
    "    layout.children = [ loading_component, logs_component ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# start_datetime = datetime.now()\n",
    "# start_datetime_timestamp_str = start_datetime.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "# # Agent training config\n",
    "# start_time = start_datetime.timestamp()\n",
    "# num_steps = 256\n",
    "# num_mini_batches = 32\n",
    "# num_training_epochs = 10\n",
    "# num_envs = 8\n",
    "# batch_size = int(num_envs * num_steps)\n",
    "# mini_batch_size = batch_size // num_mini_batches\n",
    "\n",
    "# # Reward predictor training config\n",
    "# num_frames_in_single_trajectory_video = 64\n",
    "# human_feedback_interval = 1 # Number of training iterations of PPO agent before asking for human feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from utils.video import generate_video_from_doom_play_segments\n",
    "import random\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import asyncio\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from agents.doom_ppo_agent import DoomPpoAgent\n",
    "from reward_predictors.doom_human_preference_reward_predictor import DoomHumanPreferenceRewardPredictor\n",
    "import gymnasium as gym\n",
    "\n",
    "def ask_user_for_preference(trajectory_1_video_path: str, trajectory_2_video_path: str):\n",
    "    future = asyncio.Future()\n",
    "\n",
    "    def on_trajectory_1_chosen(_):\n",
    "        future.set_result(0)\n",
    "    \n",
    "    def on_trajectory_2_chosen(_):\n",
    "        future.set_result(1)\n",
    "    \n",
    "    def on_both_trajectories_chosen(_):\n",
    "        future.set_result(0.5)\n",
    "\n",
    "    show_preference_selection_screen(trajectory_1_video_path, trajectory_2_video_path, on_trajectory_1_chosen, on_trajectory_2_chosen, on_both_trajectories_chosen)\n",
    "\n",
    "    return future\n",
    "\n",
    "def create_random_pairs(elements):\n",
    "    # Shuffle the elements array\n",
    "    shuffled_elements = elements.copy()\n",
    "    random.shuffle(shuffled_elements)\n",
    "    \n",
    "    # Create pairs of consecutive elements\n",
    "    pairs = [(shuffled_elements[i], shuffled_elements[i+1]) for i in range(0, len(shuffled_elements), 2)]\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "async def pre_train(envs: gym.vector.SyncVectorEnv, \n",
    "                    agent: DoomPpoAgent, \n",
    "                    reward_predictor: DoomHumanPreferenceRewardPredictor, \n",
    "                    num_replay_buffer_steps_per_env: int, \n",
    "                    num_preference_requests: int,\n",
    "                    num_steps_in_trajectory_video: int):\n",
    "    # Creating replay buffer for storing transition for training\n",
    "    replay_buffer = ReplayBuffer(num_replay_buffer_steps_per_env, \n",
    "                                 envs.num_envs, \n",
    "                                 envs.envs[0].raw_observation_space, \n",
    "                                 envs.single_observation_space, \n",
    "                                 envs.single_action_space)\n",
    "\n",
    "    # Initializing pre-training variables \n",
    "    global_step = 0\n",
    "    observations, infos = envs.reset()\n",
    "    terminations = [0 for _ in range(envs.num_envs)]\n",
    "    reward_sums = np.zeros(envs.num_envs, dtype=np.float32)\n",
    "    current_num_preference_requests = 0\n",
    "\n",
    "    # for i in range(num_preference_requests):\n",
    "    while current_num_preference_requests < num_preference_requests:\n",
    "        show_loading_screen(f\"Exploring Enviroment to Pre-Train Reward Predictor...\")\n",
    "        \n",
    "        for step in range(0, replay_buffer.num_steps):\n",
    "            global_step += envs.num_envs\n",
    "\n",
    "            # Getting next action and it's value\n",
    "            actions, log_probs, probs, values = agent.forward(observations)\n",
    "            values = values.flatten()\n",
    "            \n",
    "            # Performing actions in the environments\n",
    "            observations_, _, terminations_, _, infos = envs.step(actions)\n",
    "\n",
    "            # Predicting reward for the observations and the corresponding actions\n",
    "            rewards = reward_predictor.forward(observations)\n",
    "            reward_sums = reward_sums + rewards\n",
    "\n",
    "            # Saving transitions in replay buffer\n",
    "            replay_buffer[step] = (\n",
    "                np.stack(infos[\"raw_observations\"]),\n",
    "                observations,\n",
    "                actions,\n",
    "                log_probs,\n",
    "                rewards,\n",
    "                values,\n",
    "                terminations\n",
    "            )\n",
    "\n",
    "            # Saving new observation and termination status for next step\n",
    "            observations = observations_\n",
    "            terminations = terminations_\n",
    "\n",
    "        # Preparing trajectories and videos for asking user for preference\n",
    "        show_loading_screen(\"Preparing trajectories for asking user for preference...\")\n",
    "        segments = replay_buffer.get_segments(segment_length=num_steps_in_trajectory_video)\n",
    "        trajectory_pairs = create_random_pairs(segments)\n",
    "\n",
    "        for trajectory_1, trajectory_2 in trajectory_pairs:\n",
    "            generate_video_from_doom_play_segments(trajectory_1, './temp/trajectory_1.mp4')\n",
    "            generate_video_from_doom_play_segments(trajectory_2, './temp/trajectory_2.mp4')\n",
    "            \n",
    "            # Asking user for their preference\n",
    "            user_preference = await ask_user_for_preference('./temp/trajectory_1.mp4', './temp/trajectory_2.mp4')\n",
    "            log_info(f\"user_preference = {user_preference}\")\n",
    "\n",
    "            # Training reward predictor based on the user preference\n",
    "            show_loading_screen(\"Pre-training reward predictor...\")\n",
    "            try:\n",
    "                reward_predictor_training_stats = reward_predictor.train(trajectory_1, trajectory_2, user_preference)\n",
    "                log_info(f\"reward_predictor_training_loss={reward_predictor_training_stats['loss']}\")\n",
    "                current_num_preference_requests += 1\n",
    "            except Exception as error:\n",
    "                # Catch the error and print the entire stack trace\n",
    "                traceback.print_exc()\n",
    "\n",
    "async def train(envs: gym.vector.SyncVectorEnv, \n",
    "                agent: DoomPpoAgent, \n",
    "                reward_predictor: DoomHumanPreferenceRewardPredictor,\n",
    "                num_replay_buffer_steps_per_env: int,\n",
    "                num_agent_training_mini_batches: int,\n",
    "                num_agent_training_epochs: int,\n",
    "                num_steps_in_trajectory_video: int,\n",
    "                save_training_stats: bool = False):\n",
    "    # Setting up other training config \n",
    "    start_time = start_datetime.timestamp()\n",
    "    batch_size = int(envs.num_envs * num_replay_buffer_steps_per_env)\n",
    "    mini_batch_size = batch_size // num_agent_training_mini_batches\n",
    "\n",
    "    # Recording start time\n",
    "    start_datetime = datetime.now()\n",
    "    start_datetime_timestamp_str = start_datetime.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    # Creating replay buffer for storing transition for training\n",
    "    replay_buffer = ReplayBuffer(num_replay_buffer_steps_per_env, \n",
    "                                 envs.num_envs, \n",
    "                                 envs.envs[0].raw_observation_space, \n",
    "                                 envs.single_observation_space, \n",
    "                                 envs.single_action_space)\n",
    "\n",
    "    # Setting up Tensorboard for saving training stats if requested\n",
    "    if save_training_stats:\n",
    "        tensorboard_writer = SummaryWriter(f\"logs/doom_basic_level/rlhf_training_{start_datetime_timestamp_str}\")\n",
    "\n",
    "    # Initializing variables for tracking the training process \n",
    "    global_step = 0\n",
    "    agent_training_iteration = 0 \n",
    "    observations, infos = envs.reset()\n",
    "    terminations = [0 for _ in range(envs.num_envs)]\n",
    "    best_average_return = float(\"-inf\")\n",
    "    reward_sums = np.zeros(envs.num_envs, dtype=np.float32)\n",
    "    returns = []\n",
    "\n",
    "    while True:\n",
    "        show_loading_screen(f\"Training PPO Agent...\")\n",
    "        \n",
    "        for step in range(0, num_replay_buffer_steps_per_env):\n",
    "            global_step += envs.num_envs\n",
    "\n",
    "            # Getting next action and it's value\n",
    "            actions, log_probs, probs, values = agent.forward(observations)\n",
    "            values = values.flatten()\n",
    "            \n",
    "            # Performing actions in the environments\n",
    "            observations_, _, terminations_, _, infos = envs.step(actions)\n",
    "\n",
    "            # Predicting reward for the observations and the corresponding actions\n",
    "            rewards = reward_predictor.forward(observations)\n",
    "            reward_sums = reward_sums + rewards\n",
    "\n",
    "            # Saving transitions in replay buffer\n",
    "            replay_buffer[step] = (\n",
    "                np.stack(infos[\"raw_observations\"]),\n",
    "                observations,\n",
    "                actions,\n",
    "                log_probs,\n",
    "                rewards,\n",
    "                values,\n",
    "                terminations\n",
    "            )\n",
    "\n",
    "            # Saving new observation and termination status for next step\n",
    "            observations = observations_\n",
    "            terminations = terminations_\n",
    "\n",
    "            # Record episodic returns\n",
    "            for index, done in enumerate(terminations):\n",
    "                if done == 1:\n",
    "                    reward_sum = reward_sums[index]\n",
    "                    returns.append(reward_sums[index])\n",
    "                    log_info(f\"global_step={global_step}, episodic_return={reward_sums[index]}\")\n",
    "\n",
    "                    # Resetting rewards sum\n",
    "                    reward_sums[index] = 0\n",
    "\n",
    "        # Checking if the current mean is higher than previous highest mean and saving the model\n",
    "        current_mean_episodic_return = np.mean(returns)\n",
    "        log_info(f\"Current Mean Episodic Return = {current_mean_episodic_return}\")\n",
    "        if current_mean_episodic_return > best_average_return:\n",
    "            # Saving the model\n",
    "            model_save_path = f\"./models/rlhf_pipeline/training_run_{start_datetime_timestamp_str}/doom_ppo_agent/checkpoint_step_{global_step}\"\n",
    "            log_info(f\"Saving models to `{model_save_path}`...\")\n",
    "            agent.save_models(model_save_path)\n",
    "            log_info(f\"Successfully saved models to `{model_save_path}`!\")\n",
    "\n",
    "            # Saving new best average return and clearing returns arrays\n",
    "            best_average_return = current_mean_episodic_return\n",
    "            returns.clear()\n",
    "\n",
    "        # Training the agent\n",
    "        agent_training_stats = agent.train(\n",
    "            replay_buffer=replay_buffer,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            num_training_epochs=num_agent_training_epochs,\n",
    "        )\n",
    "        agent_training_iteration += 1\n",
    "        log_info(f\"SPS: {int(global_step / (time.time() - start_time))}\")\n",
    "\n",
    "        if save_training_stats:\n",
    "            # tensorboard_writer.add_scalar(\"charts/learning_rate\", training_stats.learning_rate, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/losses/value_loss\", agent_training_stats.value_loss, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/losses/policy_loss\", agent_training_stats.policy_loss, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/losses/entropy_loss\", agent_training_stats.entropy_loss, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/old_approx_kl\", agent_training_stats.old_approx_kl, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/approx_kl\", agent_training_stats.approx_kl, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/clip_fraction\", agent_training_stats.clip_fraction, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/explained_variance\", agent_training_stats.explained_variance, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "        if agent_training_iteration % human_feedback_interval == 0:\n",
    "            # Preparing trajectories and videos for asking user for preference\n",
    "            show_loading_screen(\"Preparing trajectories for asking user for preference...\")\n",
    "            segments = replay_buffer.get_segments(segment_length=num_steps_in_trajectory_video)\n",
    "            trajectory_1, trajectory_2 = random.sample(segments, k=2)\n",
    "            generate_video_from_doom_play_segments(trajectory_1, './temp/trajectory_1.mp4')\n",
    "            generate_video_from_doom_play_segments(trajectory_2, './temp/trajectory_2.mp4')\n",
    "            \n",
    "            # Asking user for their preference\n",
    "            user_preference = await ask_user_for_preference('./temp/trajectory_1.mp4', './temp/trajectory_2.mp4')\n",
    "            log_info(f\"user_preference = {user_preference}\")\n",
    "\n",
    "            # Training reward predictor based on the user preference\n",
    "            show_loading_screen(\"Training reward predictor...\")\n",
    "            try:\n",
    "                reward_predictor_training_stats = reward_predictor.train(trajectory_1, trajectory_2, user_preference)\n",
    "                log_info(f\"reward_predictor_training_loss={reward_predictor_training_stats['loss']}\")\n",
    "\n",
    "                if save_training_stats:\n",
    "                    # Writing loss value to tensorboard\n",
    "                    tensorboard_writer.add_scalar(\"ppo_agent/losses/loss\", reward_predictor_training_stats[\"loss\"], global_step)\n",
    "            except Exception as error:\n",
    "                # Catch the error and print the entire stack trace\n",
    "                traceback.print_exc()\n",
    "\n",
    "async def run_training_pipeline(envs: gym.vector.SyncVectorEnv, \n",
    "                agent: DoomPpoAgent, \n",
    "                reward_predictor: DoomHumanPreferenceRewardPredictor,\n",
    "                num_replay_buffer_steps_per_env: int,\n",
    "                num_pre_training_preference_requests: int,\n",
    "                num_agent_training_mini_batches: int,\n",
    "                num_agent_training_epochs: int,\n",
    "                num_steps_in_trajectory_video: int,\n",
    "                save_training_stats: bool = False):\n",
    "    await pre_train(envs, \n",
    "                    agent, \n",
    "                    reward_predictor, \n",
    "                    num_replay_buffer_steps_per_env=num_replay_buffer_steps_per_env,\n",
    "                    num_preference_requests=num_pre_training_preference_requests,\n",
    "                    num_steps_in_trajectory_video=num_steps_in_trajectory_video)\n",
    "\n",
    "    # await train(envs, \n",
    "    #             agent, \n",
    "    #             reward_predictor, \n",
    "    #             num_replay_buffer_steps_per_env=num_replay_buffer_steps_per_env, \n",
    "    #             num_agent_training_mini_batches=num_agent_training_mini_batches,\n",
    "    #             num_agent_training_epochs=num_agent_training_epochs,\n",
    "    #             save_training_stats=save_training_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63600c04a764fb789fe2033a5d50ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harish/opt/anaconda3/envs/doom-rlhf/lib/python3.9/site-packages/gymnasium/core.py:297: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Setting up agent and reward predictor\u001b[39;00m\n\u001b[1;32m     15\u001b[0m agent \u001b[39m=\u001b[39m DoomPpoAgent(envs\u001b[39m.\u001b[39msingle_observation_space,\n\u001b[1;32m     16\u001b[0m                      envs\u001b[39m.\u001b[39msingle_action_space,\n\u001b[1;32m     17\u001b[0m                      learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m,\n\u001b[1;32m     18\u001b[0m                      use_gpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m reward_predictor \u001b[39m=\u001b[39m DoomHumanPreferenceRewardPredictor(envs\u001b[39m.\u001b[39;49msingle_observation_space, \n\u001b[1;32m     20\u001b[0m                                                       envs\u001b[39m.\u001b[39;49msingle_action_space,\n\u001b[1;32m     21\u001b[0m                                                       learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m                                                       use_gpu\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Starting training process\u001b[39;00m\n\u001b[1;32m     25\u001b[0m event_loop \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mget_event_loop()\n",
      "File \u001b[0;32m~/Documents/Study/University of London/Semester 6/Final Project/doom-rlhf/reward_predictors/doom_human_preference_reward_predictor.py:43\u001b[0m, in \u001b[0;36mDoomHumanPreferenceRewardPredictor.__init__\u001b[0;34m(self, observation_space, hidden_size, learning_rate, drop_out, use_gpu)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, observation_space: Space, hidden_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.00025\u001b[39m, drop_out\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, use_gpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    Initialize the DoomPpoAgent.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m        use_gpu (bool): Whether to use GPU for training.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     network \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     22\u001b[0m         nn\u001b[39m.\u001b[39mConv2d(observation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m),\n\u001b[1;32m     23\u001b[0m         nn\u001b[39m.\u001b[39mLeakyReLU(negative_slope\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m),\n\u001b[1;32m     24\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm2d(\u001b[39m32\u001b[39m),\n\u001b[1;32m     25\u001b[0m         nn\u001b[39m.\u001b[39mDropout2d(drop_out),\n\u001b[1;32m     26\u001b[0m         \n\u001b[1;32m     27\u001b[0m         nn\u001b[39m.\u001b[39mConv2d(\u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[1;32m     28\u001b[0m         nn\u001b[39m.\u001b[39mLeakyReLU(negative_slope\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m),\n\u001b[1;32m     29\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm2d(\u001b[39m64\u001b[39m),\n\u001b[1;32m     30\u001b[0m         nn\u001b[39m.\u001b[39mDropout2d(drop_out),\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m         nn\u001b[39m.\u001b[39mConv2d(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m     33\u001b[0m         nn\u001b[39m.\u001b[39mLeakyReLU(negative_slope\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m),\n\u001b[1;32m     34\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm2d(\u001b[39m64\u001b[39m),\n\u001b[1;32m     35\u001b[0m         nn\u001b[39m.\u001b[39mDropout2d(drop_out),\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m         nn\u001b[39m.\u001b[39mConv2d(\u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m     38\u001b[0m         nn\u001b[39m.\u001b[39mLeakyReLU(negative_slope\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m),\n\u001b[1;32m     39\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm2d(\u001b[39m64\u001b[39m),\n\u001b[1;32m     40\u001b[0m         nn\u001b[39m.\u001b[39mDropout2d(drop_out),\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m         nn\u001b[39m.\u001b[39mFlatten(),\n\u001b[0;32m---> 43\u001b[0m         nn\u001b[39m.\u001b[39;49mLinear(in_features\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m11\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m16\u001b[39;49m, out_features\u001b[39m=\u001b[39;49mhidden_size),\n\u001b[1;32m     44\u001b[0m         nn\u001b[39m.\u001b[39mReLU(),\n\u001b[1;32m     45\u001b[0m         nn\u001b[39m.\u001b[39mLinear(hidden_size, \u001b[39m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     48\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(network, observation_space\u001b[39m.\u001b[39mshape, learning_rate, use_gpu)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/doom-rlhf/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "%gui asyncio\n",
    "\n",
    "from utils.env import make_vizdoom_env\n",
    "\n",
    "# Displaying UI and showing only the loading screen\n",
    "clear_output()\n",
    "display(layout)\n",
    "hide_all_screens()\n",
    "\n",
    "# Initializing environments with only the first enviroment rendering\n",
    "num_envs = 8\n",
    "envs = gym.vector.SyncVectorEnv([ make_vizdoom_env('envs/vizdoom/scenarios/basic.cfg', render_mode=\"human\" if i == 0 else None) for i in range(num_envs)])\n",
    "\n",
    "# Setting up agent and reward predictor\n",
    "agent = DoomPpoAgent(envs.single_observation_space,\n",
    "                     envs.single_action_space,\n",
    "                     learning_rate=0.0001,\n",
    "                     use_gpu=True)\n",
    "reward_predictor = DoomHumanPreferenceRewardPredictor(envs.single_observation_space, \n",
    "                                                      envs.single_action_space,\n",
    "                                                      learning_rate=1e-4,\n",
    "                                                      use_gpu=True)\n",
    "\n",
    "# Starting training process\n",
    "event_loop = asyncio.get_event_loop()\n",
    "training_task = event_loop.create_task(run_training_pipeline(envs, \n",
    "                                                             agent, \n",
    "                                                             reward_predictor, \n",
    "                                                             num_replay_buffer_steps_per_env=256, \n",
    "                                                             num_pre_training_preference_requests=500,\n",
    "                                                             num_agent_training_mini_batches=32,\n",
    "                                                             num_agent_training_epochs=10,\n",
    "                                                             num_steps_in_trajectory_video=64,\n",
    "                                                             save_training_stats=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom-rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
