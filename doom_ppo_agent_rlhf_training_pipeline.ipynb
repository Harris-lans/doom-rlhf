{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF Trainining Pipeline for PPO Agent to Play Levels from the Doom Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_datetime = datetime.now()\n",
    "start_datetime_timestamp_str = start_datetime.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "# Agent training config\n",
    "start_time = start_datetime.timestamp()\n",
    "num_steps = 256\n",
    "num_mini_batches = 32\n",
    "num_training_epochs = 10\n",
    "num_envs = 8\n",
    "batch_size = int(num_envs * num_steps)\n",
    "mini_batch_size = batch_size // num_mini_batches\n",
    "\n",
    "# Reward predictor training config\n",
    "num_batches_before_collecting_feedback = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\core.py:297: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\core.py:297: UserWarning: \u001b[33mWARN: env.raw_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.raw_observation_space` for environment variables or `env.get_attr('raw_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from agents.doom_ppo_agent import DoomPpoAgent\n",
    "from reward_predictors.doom_human_preference_reward_predictor import DoomHumanPreferenceRewardPredictor\n",
    "from utils.time import current_timestamp_ms\n",
    "import gymnasium as gym\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "from utils.env import make_vizdoom_env\n",
    "\n",
    "# Initializing environments\n",
    "envs = gym.vector.SyncVectorEnv([ make_vizdoom_env('envs/vizdoom/scenarios/basic.cfg') for i in range(num_envs)])\n",
    "\n",
    "# Setting up agent and reward predictor\n",
    "agent = DoomPpoAgent(envs.single_observation_space,\n",
    "                     envs.single_action_space,\n",
    "                     learning_rate=0.0001,\n",
    "                     use_gpu=True)\n",
    "reward_predictor = DoomHumanPreferenceRewardPredictor(envs.single_observation_space, \n",
    "                                                      envs.single_action_space, \n",
    "                                                      use_gpu=True)\n",
    "\n",
    "# Creating replay buffer for storing steps\n",
    "replay_buffer = ReplayBuffer(num_steps, \n",
    "                             num_envs, \n",
    "                             envs.envs[0].raw_observation_space, \n",
    "                             envs.single_observation_space, \n",
    "                             envs.single_action_space)\n",
    "\n",
    "# Setting up debugging for Tensorboard\n",
    "tensorboard_writer = SummaryWriter(f\"logs/doom_basic_level/rlhf_training_{start_datetime_timestamp_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, clear_output\n",
    "from typing import Callable\n",
    "\n",
    "# Function for rendering widgets\n",
    "def create_html_heading(text, level=1, centered=True):\n",
    "    html_tag = f'<h{level}>{text}</h{level}>'\n",
    "    layout = widgets.Layout(display='flex', justify_content='center') if centered else None\n",
    "    return widgets.HTML(html_tag, layout=layout)\n",
    "\n",
    "def create_video_player(video_path: str):\n",
    "    return widgets.Video()\n",
    "    return widgets.Video.from_file(video_path)\n",
    "\n",
    "def create_button(description, tooltip):\n",
    "    return widgets.Button(description=description, disabled=False, button_style='',\n",
    "                          tooltip=tooltip, layout=widgets.Layout(display='flex', justify_content='center'))\n",
    "\n",
    "def create_loading_spinner():\n",
    "    css = \"\"\"\n",
    "    .loader {\n",
    "        border: 8px solid #f3f3f3;\n",
    "        border-top: 8px solid #3498db;\n",
    "        border-radius: 50%;\n",
    "        width: 100px;\n",
    "        height: 100px;\n",
    "        animation: spin 2s linear infinite;\n",
    "    }\n",
    "\n",
    "    @keyframes spin {\n",
    "        0% { transform: rotate(0deg); }\n",
    "        100% { transform: rotate(360deg); }\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Create the custom CSS and apply it to the heading\n",
    "    heading_style = widgets.HTML(f'<style>{css}</style>')\n",
    "    # Create the loading spinner widget\n",
    "    loading_spinner = widgets.HTML('<div class=\"loader\"></div>', layout=widgets.Layout(display='flex', justify_content='center'))\n",
    "    \n",
    "    return heading_style, loading_spinner\n",
    "\n",
    "def create_preference_selection_screen(trajectory_1_video_path: str, trajectory_2_video_path: str, on_trajectory_1_chosen: Callable, on_trajectory_2_chosen: Callable, on_both_trajectories_chosen: Callable):\n",
    "    # Creating GUI Components\n",
    "    window_label = create_html_heading('Which trajectory do you prefer?', centered=True)\n",
    "    trajectory_1_label = create_html_heading('Trajectory 1', level=2, centered=True)\n",
    "    trajectory_2_label = create_html_heading('Trajectory 2', level=2, centered=True)\n",
    "    trajectory_1_video_player = create_video_player(trajectory_1_video_path)\n",
    "    trajectory_2_video_player = create_video_player(trajectory_2_video_path)\n",
    "    prefer_trajectory_1_button = create_button('Select 1', 'Choose Trajectory 1')\n",
    "    prefer_trajectory_2_button = create_button('Select 2', 'Choose Trajectory 2')\n",
    "    prefer_both_trajectories_button = create_button('Both', 'Choose both Trajectories')\n",
    "    \n",
    "    prefer_trajectory_1_button.on_click(on_trajectory_1_chosen)\n",
    "    prefer_trajectory_2_button.on_click(on_trajectory_2_chosen)\n",
    "    prefer_both_trajectories_button.on_click(on_both_trajectories_chosen)\n",
    "\n",
    "    # Rendering components\n",
    "    preference_selection_layout = widgets.VBox([\n",
    "        window_label,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                trajectory_1_label,\n",
    "                trajectory_1_video_player,\n",
    "                prefer_trajectory_1_button\n",
    "            ], layout=widgets.Layout(align_items='center', justify_content='space-between')),\n",
    "            widgets.VBox([\n",
    "                trajectory_2_label,\n",
    "                trajectory_2_video_player,\n",
    "                prefer_trajectory_2_button\n",
    "            ], layout=widgets.Layout(align_items='center', justify_content='space-between'))\n",
    "        ], layout=widgets.Layout(justify_content='space-between', width='700px', height='300px')),\n",
    "        widgets.HBox([\n",
    "            prefer_both_trajectories_button\n",
    "        ])\n",
    "    ], layout=widgets.Layout(align_items='center', padding='15px'))\n",
    "\n",
    "    return preference_selection_layout\n",
    "\n",
    "def create_loading_screen(message):\n",
    "    # Create the heading widget and apply the style\n",
    "    heading = create_html_heading(message, centered=True)\n",
    "    # Create the loading spinner widget\n",
    "    heading_style, loading_spinner = create_loading_spinner()\n",
    "\n",
    "    # Create the outer VBox layout with widgets\n",
    "    outer_vbox = widgets.VBox([heading_style, heading, loading_spinner], layout=widgets.Layout(align_items='center', justify_content='center', padding='15px', height='435px'))\n",
    "\n",
    "    return outer_vbox\n",
    "\n",
    "loading_screen = create_loading_screen('Training agent...')\n",
    "preference_selection_screen = create_preference_selection_screen('', '', lambda _: print(\"Trajectory 1 chosen\"), lambda _: print(\"Trajectory 2 chosen\"), lambda _: print(\"Both trajectories chosen\"))\n",
    "layout = widgets.VBox([])\n",
    "\n",
    "def hide_all_screens():\n",
    "    layout.children = []\n",
    "\n",
    "def show_preference_selection_screen():\n",
    "    layout.children = [ preference_selection_screen ]\n",
    "\n",
    "def show_loading_screen():\n",
    "    layout.children = [ loading_screen ]\n",
    "\n",
    "# display(layout)\n",
    "# hide_all_screens()\n",
    "# show_loading_screen()\n",
    "# time.sleep(3)\n",
    "# show_preference_selection_screen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=64, episodic_return=1.3516228199005127\n",
      "global_step=80, episodic_return=2.1228954792022705\n",
      "global_step=88, episodic_return=2.26633620262146\n",
      "global_step=96, episodic_return=2.9798216819763184\n",
      "global_step=144, episodic_return=0.7902706265449524\n",
      "global_step=152, episodic_return=2.3844170570373535\n",
      "global_step=160, episodic_return=1.7644915580749512\n",
      "global_step=320, episodic_return=4.686269283294678\n",
      "global_step=376, episodic_return=1.3535767793655396\n",
      "global_step=400, episodic_return=4.815674304962158\n",
      "global_step=480, episodic_return=2.5093533992767334\n",
      "global_step=504, episodic_return=3.564544916152954\n",
      "global_step=544, episodic_return=1.4694359302520752\n",
      "global_step=568, episodic_return=1.7664660215377808\n",
      "global_step=1224, episodic_return=16.17155647277832\n",
      "global_step=1272, episodic_return=1.324235439300537\n",
      "global_step=2032, episodic_return=50.41633224487305\n",
      "Current Mean Episodic Return = 5.984547138214111\n",
      "Saving models...\n",
      "Directory './models/rlhf_pipeline/training_run_2023_07_31_18_31_28/doom_ppo_agent/checkpoint_step_2048' created!\n",
      "Successfully saved models!\n",
      "SPS: 36\n",
      "global_step=2096, episodic_return=1.5887047052383423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mflatten()\n\u001b[0;32m     25\u001b[0m \u001b[39m# Performing actions in the environments\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m observations_, _, terminations_, _, infos \u001b[39m=\u001b[39m envs\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Predicting reward for the observations and the corresponding actions\u001b[39;00m\n\u001b[0;32m     29\u001b[0m rewards \u001b[39m=\u001b[39m reward_predictor\u001b[39m.\u001b[39mforward(observations, actions)\n",
      "File \u001b[1;32mc:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\vector\\vector_env.py:203\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m    {}\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 203\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:149\u001b[0m, in \u001b[0;36mSyncVectorEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m observations, infos \u001b[39m=\u001b[39m [], {}\n\u001b[0;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m i, (env, action) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actions)):\n\u001b[0;32m    143\u001b[0m     (\n\u001b[0;32m    144\u001b[0m         observation,\n\u001b[0;32m    145\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rewards[i],\n\u001b[0;32m    146\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_terminateds[i],\n\u001b[0;32m    147\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncateds[i],\n\u001b[0;32m    148\u001b[0m         info,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_terminateds[i] \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncateds[i]:\n\u001b[0;32m    152\u001b[0m         old_observation, old_info \u001b[39m=\u001b[39m observation, info\n",
      "File \u001b[1;32mc:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\wrappers\\frame_stack.py:179\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m    171\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[0;32m    173\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes\u001b[39m.\u001b[39mappend(observation)\n\u001b[0;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(\u001b[39mNone\u001b[39;00m), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\core.py:508\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    505\u001b[0m     \u001b[39mself\u001b[39m, action: ActType\n\u001b[0;32m    506\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    507\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\core.py:508\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    505\u001b[0m     \u001b[39mself\u001b[39m, action: ActType\n\u001b[0;32m    506\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    507\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32me:\\Study\\University of London\\Semester 6\\Final Project\\doom-rlhf\\envs\\wrappers\\record_observations.py:19\u001b[0m, in \u001b[0;36mRecordObservations.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     18\u001b[0m     \u001b[39m# Perform the action and get the next state and other information from the environment\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     observations, rewards, terminations, truncations, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     21\u001b[0m     infos[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo_key] \u001b[39m=\u001b[39m observations\n\u001b[0;32m     23\u001b[0m     \u001b[39m# Return the next state and other information as usual\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\wrappers\\record_episode_statistics.py:94\u001b[0m, in \u001b[0;36mRecordEpisodeStatistics.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     87\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     (\n\u001b[0;32m     89\u001b[0m         observations,\n\u001b[0;32m     90\u001b[0m         rewards,\n\u001b[0;32m     91\u001b[0m         terminations,\n\u001b[0;32m     92\u001b[0m         truncations,\n\u001b[0;32m     93\u001b[0m         infos,\n\u001b[1;32m---> 94\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m     96\u001b[0m         infos, \u001b[39mdict\u001b[39m\n\u001b[0;32m     97\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`info` dtype is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(infos)\u001b[39m}\u001b[39;00m\u001b[39m while supported dtype is `dict`. This may be due to usage of other wrappers in the wrong order.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_returns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n",
      "File \u001b[1;32me:\\Study\\University of London\\Semester 6\\Final Project\\doom-rlhf\\envs\\vizdoom_env.py:133\u001b[0m, in \u001b[0;36mVizdoomEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCall `reset` before using `step` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m env_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__build_env_action(action)\n\u001b[1;32m--> 133\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame\u001b[39m.\u001b[39;49mmake_action(env_action, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframe_skip)\n\u001b[0;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mget_state()\n\u001b[0;32m    135\u001b[0m terminated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame\u001b[39m.\u001b[39mis_episode_finished()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from utils.video import generate_video_from_doom_play_segments\n",
    "import random\n",
    "\n",
    "global_step = 0\n",
    "observations, infos = envs.reset()\n",
    "terminations = [0 for _ in range(num_envs)]\n",
    "best_average_return = float(\"-inf\")\n",
    "reward_sums = np.zeros(num_envs, dtype=np.float32)\n",
    "returns = []\n",
    "segments = []\n",
    "\n",
    "while True:\n",
    "    # Calculating learning rate annealing coefficient\n",
    "    # learning_rate_anneal_coef = 1.0 - (update - 1.0) / num_updates\n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += num_envs\n",
    "\n",
    "        # Getting next action and it's value\n",
    "        actions, log_probs, probs, values = agent.forward(observations)\n",
    "        values = values.flatten()\n",
    "        \n",
    "        # Performing actions in the environments\n",
    "        observations_, _, terminations_, _, infos = envs.step(actions)\n",
    "\n",
    "        # Predicting reward for the observations and the corresponding actions\n",
    "        rewards = reward_predictor.forward(observations, actions)\n",
    "        reward_sums = reward_sums + rewards\n",
    "\n",
    "        # Saving transitions in replay buffer\n",
    "        replay_buffer[step] = (\n",
    "            np.stack(infos[\"raw_observations\"]),\n",
    "            observations,\n",
    "            actions,\n",
    "            log_probs,\n",
    "            rewards,\n",
    "            values,\n",
    "            terminations\n",
    "        )\n",
    "\n",
    "        # Saving new observation and done status for next step\n",
    "        observations = observations_\n",
    "        terminations = terminations_\n",
    "\n",
    "        # Record episodic returns\n",
    "        for index, done in enumerate(terminations):\n",
    "            if done == 1:\n",
    "                reward_sum = reward_sums[index]\n",
    "                returns.append(reward_sums[index])\n",
    "                print(f\"global_step={global_step}, episodic_return={reward_sums[index]}\")\n",
    "\n",
    "                # Resetting rewards sum\n",
    "                reward_sums[index] = 0\n",
    "\n",
    "    # Saving a random segment\n",
    "    segments = replay_buffer.get_episodic_segments(300)\n",
    "    generate_video_from_doom_play_segments(random.choice(segments), './temp/trajectory-1.mp4', 30)\n",
    "\n",
    "    # Checking if the current mean is higher than previous highest mean and saving the model\n",
    "    current_mean_episodic_return = np.mean(returns)\n",
    "    print(f\"Current Mean Episodic Return = {current_mean_episodic_return}\")\n",
    "    if current_mean_episodic_return > best_average_return:\n",
    "        # Saving the model\n",
    "        agent.save_models(\n",
    "            f\"./models/rlhf_pipeline/training_run_{start_datetime_timestamp_str}/doom_ppo_agent/checkpoint_step_{global_step}\"\n",
    "        )\n",
    "\n",
    "        # Saving new best average return and clearing returns arrays\n",
    "        best_average_return = current_mean_episodic_return\n",
    "        returns.clear()\n",
    "\n",
    "    # Training the agent\n",
    "    training_stats = agent.train(\n",
    "        replay_buffer=replay_buffer,\n",
    "        # learning_rate_anneal_coef=learning_rate_anneal_coef,\n",
    "        mini_batch_size=mini_batch_size,\n",
    "        num_training_epochs=num_training_epochs,\n",
    "    )\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "\n",
    "    # tensorboard_writer.add_scalar(\"charts/learning_rate\", training_stats.learning_rate, global_step)\n",
    "    tensorboard_writer.add_scalar(\"ppo_agent/losses/value_loss\", training_stats.value_loss, global_step)\n",
    "    tensorboard_writer.add_scalar(\"ppo_agent/losses/policy_loss\", training_stats.policy_loss, global_step)\n",
    "    tensorboard_writer.add_scalar(\"ppo_agent/losses/entropy_loss\", training_stats.entropy_loss, global_step)\n",
    "    tensorboard_writer.add_scalar(\"ppo_agent/charts/old_approx_kl\", training_stats.old_approx_kl, global_step)\n",
    "    tensorboard_writer.add_scalar(\"ppo_agent/charts/approx_kl\", training_stats.approx_kl, global_step)\n",
    "    tensorboard_writer.add_scalar(\"ppo_agent/charts/clip_fraction\", training_stats.clip_fraction, global_step)\n",
    "    tensorboard_writer.add_scalar(\"ppo_agent/charts/explained_variance\", training_stats.explained_variance, global_step)\n",
    "    tensorboard_writer.add_scalar(\"ppo_agent/charts/SPS\", int(global_step / (time.time() - start_time)), global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# observations = random.choice(segments).observations\n",
    "# plt.imshow(cv2.cvtColor(observations[0][0], cv2.COLOR_GRAY2RGB))\n",
    "generate_video_from_doom_play_segments(random.choice(segments), './temp/trajectory-1.mp4', 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom-rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
