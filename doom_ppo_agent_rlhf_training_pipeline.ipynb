{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF Trainining Pipeline for PPO Agent to Play Levels from the Doom Game\n",
    "\n",
    "## Creating UI Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from typing import Callable\n",
    "from datetime import datetime\n",
    "\n",
    "# Function for rendering widgets\n",
    "def create_html_heading(text, level=1, centered=True):\n",
    "    html_tag = f'<h{level}>{text}</h{level}>'\n",
    "    layout = widgets.Layout(display='flex', justify_content='center') if centered else None\n",
    "    return widgets.HTML(html_tag, layout=layout)\n",
    "\n",
    "def create_video_player(video_path: str):\n",
    "    return widgets.Video.from_file(video_path)\n",
    "\n",
    "def create_button(description, tooltip, button_style=''):\n",
    "    return widgets.Button(description=description, disabled=False, button_style=button_style,\n",
    "                          tooltip=tooltip, layout=widgets.Layout(display='flex', justify_content='center', margin='4px'))\n",
    "\n",
    "def create_loading_spinner():\n",
    "    css = \"\"\"\n",
    "    .loader {\n",
    "        border: 8px solid #f3f3f3;\n",
    "        border-top: 8px solid #3498db;\n",
    "        border-radius: 50%;\n",
    "        width: 100px;\n",
    "        height: 100px;\n",
    "        animation: spin 2s linear infinite;\n",
    "    }\n",
    "\n",
    "    @keyframes spin {\n",
    "        0% { transform: rotate(0deg); }\n",
    "        100% { transform: rotate(360deg); }\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Create the custom CSS and apply it to the heading\n",
    "    heading_style = widgets.HTML(f'<style>{css}</style>')\n",
    "    # Create the loading spinner widget\n",
    "    loading_spinner = widgets.HTML('<div class=\"loader\"></div>', layout=widgets.Layout(display='flex', justify_content='center'))\n",
    "    \n",
    "    return heading_style, loading_spinner\n",
    "\n",
    "def create_preference_selection_component(trajectory_1_video_path: str, trajectory_2_video_path: str, on_trajectory_1_chosen: Callable, on_trajectory_2_chosen: Callable, on_both_trajectories_chosen: Callable):\n",
    "    # Creating GUI Components\n",
    "    window_label = create_html_heading('Which trajectory do you prefer?', centered=True)\n",
    "    trajectory_1_label = create_html_heading('Trajectory 1', level=2, centered=True)\n",
    "    trajectory_2_label = create_html_heading('Trajectory 2', level=2, centered=True)\n",
    "    trajectory_1_video_player = create_video_player(trajectory_1_video_path)\n",
    "    trajectory_2_video_player = create_video_player(trajectory_2_video_path)\n",
    "    prefer_trajectory_1_button = create_button('Select 1', 'You prefer Trajectory 1')\n",
    "    prefer_trajectory_2_button = create_button('Select 2', 'You prefer Trajectory 2')\n",
    "    prefer_both_trajectories_button = create_button('Both', 'You prefer both Trajectories')\n",
    "    \n",
    "    prefer_trajectory_1_button.on_click(on_trajectory_1_chosen)\n",
    "    prefer_trajectory_2_button.on_click(on_trajectory_2_chosen)\n",
    "    prefer_both_trajectories_button.on_click(on_both_trajectories_chosen)\n",
    "\n",
    "    window_label.layout.flex = '1'\n",
    "\n",
    "    # Rendering components\n",
    "    preference_selection_layout = widgets.VBox([\n",
    "        window_label,\n",
    "        widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                trajectory_1_label,\n",
    "                trajectory_1_video_player,\n",
    "                prefer_trajectory_1_button\n",
    "            ], layout=widgets.Layout(align_items='center', justify_content='space-between', width='40%', height='100%')),\n",
    "            widgets.VBox([\n",
    "                trajectory_2_label,\n",
    "                trajectory_2_video_player,\n",
    "                prefer_trajectory_2_button\n",
    "            ], layout=widgets.Layout(align_items='center', justify_content='space-between', width='40%', height='100%'))\n",
    "        ], layout=widgets.Layout(justify_content='space-between', flex='4', width='100%')),\n",
    "        widgets.HBox([\n",
    "            prefer_both_trajectories_button\n",
    "        ], layout=widgets.Layout(justify_content='space-between', flex='1'))\n",
    "    ], layout=widgets.Layout(align_items='center', justify_content='space-between'))\n",
    "\n",
    "    return preference_selection_layout\n",
    "\n",
    "def create_loading_component(message):\n",
    "    # Create the heading widget and apply the style\n",
    "    heading = create_html_heading(message, centered=True)\n",
    "    # Create the loading spinner widget\n",
    "    heading_style, loading_spinner = create_loading_spinner()\n",
    "\n",
    "    # Create the outer VBox layout with widgets\n",
    "    outer_vbox = widgets.VBox([heading_style, heading, loading_spinner], layout=widgets.Layout(align_items='center', justify_content='center'))\n",
    "\n",
    "    return outer_vbox\n",
    "\n",
    "def create_logs_component():\n",
    "    logs_label = widgets.HTML('<h1>Logs</h1>')\n",
    "    logs_output = widgets.Output(description=\"Output\")\n",
    "    logs_layout = widgets.VBox([\n",
    "            logs_label,\n",
    "            widgets.Box([\n",
    "                logs_output\n",
    "            ], layout=widgets.Layout(flex='1', width='98%', overflow_y='scroll'))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return logs_layout, logs_output\n",
    "\n",
    "logs_component, logs_output = create_logs_component()\n",
    "layout = widgets.HBox([])\n",
    "\n",
    "def log_info(message: str):\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    message = f\"[{current_timestamp}][INFO] {message}\"\n",
    "    logs_output.append_stdout(f\"{message}\\n\")\n",
    "    # print(message)\n",
    "\n",
    "def log_error(message: str):\n",
    "    current_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    message = f\"[{current_timestamp}][ERROR] {message}\\n\"\n",
    "    logs_output.append_stderr(message)\n",
    "\n",
    "def hide_all_screens():\n",
    "    layout.children = []\n",
    "\n",
    "def show_preference_selection_screen(trajectory_1_video_path: str, trajectory_2_video_path: str, on_trajectory_1_chosen: Callable, on_trajectory_2_chosen: Callable, on_both_trajectories_chosen: Callable):\n",
    "    preference_selection_component = create_preference_selection_component(trajectory_1_video_path, trajectory_2_video_path, on_trajectory_1_chosen, on_trajectory_2_chosen, on_both_trajectories_chosen)\n",
    "\n",
    "    # Updating layout of components to fit the screen layout\n",
    "    preference_selection_component.layout.padding = '15px' \n",
    "    preference_selection_component.layout.margin = '7.5px' \n",
    "    preference_selection_component.layout.border = '3px dashed cornflowerblue' \n",
    "    preference_selection_component.layout.flex = '1' \n",
    "    preference_selection_component.layout.height = '600px'\n",
    "    \n",
    "    logs_component.layout.padding = '15px'\n",
    "    logs_component.layout.margin = '7.5px' \n",
    "    logs_component.layout.border = '3px dashed cornflowerblue' \n",
    "    logs_component.layout.flex = '1' \n",
    "    logs_component.layout.height = '600px'\n",
    "\n",
    "    layout.children = [ preference_selection_component, logs_component ]\n",
    "\n",
    "def show_loading_screen(message: str):\n",
    "    loading_component = create_loading_component(message)\n",
    "\n",
    "    # Updating layout of components to fit the screen layout\n",
    "    loading_component.layout.padding = '15px'\n",
    "    loading_component.layout.margin = '7.5px' \n",
    "    loading_component.layout.border = '3px dashed cornflowerblue' \n",
    "    loading_component.layout.flex = '1' \n",
    "    loading_component.layout.height = '600px'\n",
    "    \n",
    "    logs_component.layout.padding = '15px'\n",
    "    logs_component.layout.margin = '7.5px' \n",
    "    logs_component.layout.border = '3px dashed cornflowerblue' \n",
    "    logs_component.layout.flex = '1' \n",
    "    logs_component.layout.height = '600px'\n",
    "\n",
    "    layout.children = [ loading_component, logs_component ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from utils.video import generate_video_from_doom_play_segments\n",
    "import random\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import asyncio\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from agents.doom_ppo_agent import DoomPpoAgent\n",
    "from reward_predictors.doom_human_preference_reward_predictor import DoomHumanPreferenceRewardPredictor\n",
    "import gymnasium as gym\n",
    "\n",
    "# Recording start time\n",
    "start_datetime = datetime.now()\n",
    "start_datetime_timestamp_str = start_datetime.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "start_time = start_datetime.timestamp()\n",
    "\n",
    "def ask_user_for_preference(trajectory_1_video_path: str, trajectory_2_video_path: str):\n",
    "    future = asyncio.Future()\n",
    "\n",
    "    def on_trajectory_1_chosen(_):\n",
    "        future.set_result(0)\n",
    "    \n",
    "    def on_trajectory_2_chosen(_):\n",
    "        future.set_result(1)\n",
    "    \n",
    "    def on_both_trajectories_chosen(_):\n",
    "        future.set_result(0.5)\n",
    "\n",
    "    show_preference_selection_screen(trajectory_1_video_path, trajectory_2_video_path, on_trajectory_1_chosen, on_trajectory_2_chosen, on_both_trajectories_chosen)\n",
    "\n",
    "    return future\n",
    "\n",
    "def create_random_pairs(elements):\n",
    "    # Shuffle the elements array\n",
    "    shuffled_elements = elements.copy()\n",
    "    random.shuffle(shuffled_elements)\n",
    "    \n",
    "    # Create pairs of consecutive elements\n",
    "    pairs = [(shuffled_elements[i], shuffled_elements[i+1]) for i in range(0, len(shuffled_elements), 2)]\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "async def pre_train(envs: gym.vector.SyncVectorEnv, \n",
    "                    agent: DoomPpoAgent, \n",
    "                    reward_predictor: DoomHumanPreferenceRewardPredictor,\n",
    "                    pipeline_args: dict,\n",
    "                    reward_predictor_args: dict):\n",
    "    # Creating replay buffer for storing transition for training\n",
    "    replay_buffer = ReplayBuffer(pipeline_args.get('env_replay_buffer_size'), \n",
    "                                 envs.num_envs, \n",
    "                                 envs.envs[0].raw_observation_space, \n",
    "                                 envs.single_observation_space, \n",
    "                                 envs.single_action_space)\n",
    "\n",
    "    # Initializing pre-training variables \n",
    "    global_step = 0\n",
    "    observations, infos = envs.reset()\n",
    "    terminations = [0 for _ in range(envs.num_envs)]\n",
    "    current_num_preference_requests = 0\n",
    "    all_preferences_collected = False\n",
    "\n",
    "    while not all_preferences_collected:\n",
    "        show_loading_screen(f\"Exploring Enviroment to Pre-Train Reward Predictor...\")\n",
    "        \n",
    "        for step in range(0, replay_buffer.num_steps_per_env):\n",
    "            global_step += envs.num_envs\n",
    "\n",
    "            # Getting next action and it's value\n",
    "            actions, log_probs, probs, values = agent.forward(observations)\n",
    "            values = values.flatten()\n",
    "            \n",
    "            # Performing actions in the environments\n",
    "            observations_, _, terminations_, _, infos = envs.step(actions)\n",
    "\n",
    "            # Predicting reward for the observations and the corresponding actions\n",
    "            rewards = reward_predictor.forward(observations)\n",
    "\n",
    "            # Saving transitions in replay buffer\n",
    "            replay_buffer[step] = (\n",
    "                np.stack(infos[\"raw_observations\"]),\n",
    "                observations,\n",
    "                actions,\n",
    "                log_probs,\n",
    "                rewards,\n",
    "                values,\n",
    "                terminations\n",
    "            )\n",
    "\n",
    "            # Saving new observation and termination status for next step\n",
    "            observations = observations_\n",
    "            terminations = terminations_\n",
    "\n",
    "        # Preparing trajectories and videos for asking user for preference\n",
    "        show_loading_screen(\"Preparing trajectories for asking user for preference...\")\n",
    "        segments = replay_buffer.get_segments(segment_length=pipeline_args.get('num_frames_in_trajectory_video'))\n",
    "        trajectory_pairs = create_random_pairs(segments)\n",
    "\n",
    "        # Collecting human preferences and training\n",
    "        for trajectory_1, trajectory_2 in trajectory_pairs:\n",
    "            generate_video_from_doom_play_segments(trajectory_1, './temp/trajectory_1.mp4')\n",
    "            generate_video_from_doom_play_segments(trajectory_2, './temp/trajectory_2.mp4')\n",
    "            \n",
    "            # Asking user for their preference\n",
    "            user_preference = await ask_user_for_preference('./temp/trajectory_1.mp4', './temp/trajectory_2.mp4')\n",
    "\n",
    "            # Training reward predictor based on the user preference\n",
    "            log_info(f\"user_preference = {user_preference}\")\n",
    "            show_loading_screen(\"Pre-training reward predictor...\")\n",
    "            reward_predictor_training_stats = reward_predictor.train(trajectory_1, trajectory_2, user_preference, reward_predictor_args.get('num_training_epochs'))\n",
    "            log_info(f\"reward_predictor_training_loss={reward_predictor_training_stats['loss']}\")\n",
    "            current_num_preference_requests += 1\n",
    "\n",
    "            log_info(f\"Recieved {current_num_preference_requests} preferences!\")\n",
    "\n",
    "            # Exiting the preference collection loop if the target number of requests is met\n",
    "            if current_num_preference_requests >= pipeline_args.get('num_pre_train_requests'):\n",
    "                all_preferences_collected = True\n",
    "                break\n",
    "\n",
    "async def train(envs: gym.vector.SyncVectorEnv, \n",
    "                agent: DoomPpoAgent, \n",
    "                reward_predictor: DoomHumanPreferenceRewardPredictor,\n",
    "                pipeline_args: dict,\n",
    "                agent_args: dict,\n",
    "                reward_predictor_args: dict):\n",
    "    # Setting up other training config \n",
    "    batch_size = int(envs.num_envs * pipeline_args.get('env_replay_buffer_size'))\n",
    "    mini_batch_size = batch_size // agent_args.get('num_mini_batches')\n",
    "    num_updates = pipeline_args.get('total_timesteps') // batch_size\n",
    "\n",
    "    print(f\"num_training_epochs={agent_args.get('num_training_epochs')}\")\n",
    "\n",
    "    # Creating replay buffer for storing transition for training\n",
    "    replay_buffer = ReplayBuffer(pipeline_args.get('env_replay_buffer_size'), \n",
    "                                 envs.num_envs, \n",
    "                                 envs.envs[0].raw_observation_space, \n",
    "                                 envs.single_observation_space, \n",
    "                                 envs.single_action_space)\n",
    "\n",
    "    # Setting up Tensorboard for saving training stats if requested\n",
    "    if pipeline_args.get('track_stats'):\n",
    "        tensorboard_writer = SummaryWriter(f\"logs/doom_basic_level/rlhf_training_{start_datetime_timestamp_str}\")\n",
    "\n",
    "    # Initializing variables for tracking the training process \n",
    "    global_step = 0\n",
    "    agent_training_iteration = 0 \n",
    "    observations, infos = envs.reset()\n",
    "    terminations = [0 for _ in range(envs.num_envs)]\n",
    "    best_average_return = float(\"-inf\")\n",
    "    reward_predictor_reward_sums = np.zeros(envs.num_envs, dtype=np.float32)\n",
    "    env_reward_sums = np.zeros(envs.num_envs, dtype=np.float32)\n",
    "    reward_predictor_episodic_returns = []\n",
    "\n",
    "\n",
    "    for update in range(1, num_updates + 1):\n",
    "        show_loading_screen(f\"Training PPO Agent...\")\n",
    "        \n",
    "        for step in range(0, replay_buffer.num_steps_per_env):\n",
    "            global_step += envs.num_envs\n",
    "\n",
    "            # Getting next action and it's value\n",
    "            actions, log_probs, probs, values = agent.forward(observations)\n",
    "            values = values.flatten()\n",
    "            \n",
    "            # Performing actions in the environments\n",
    "            observations_, env_rewards, terminations_, _, infos = envs.step(actions)\n",
    "\n",
    "            # Predicting reward for the observations and the corresponding actions\n",
    "            rewards = reward_predictor.forward(observations)\n",
    "            reward_predictor_reward_sums = reward_predictor_reward_sums + rewards\n",
    "            env_reward_sums = env_reward_sums + env_rewards\n",
    "\n",
    "            # Saving transitions in replay buffer\n",
    "            replay_buffer[step] = (\n",
    "                np.stack(infos[\"raw_observations\"]),\n",
    "                observations,\n",
    "                actions,\n",
    "                log_probs,\n",
    "                rewards,\n",
    "                values,\n",
    "                terminations\n",
    "            )\n",
    "\n",
    "            # Saving new observation and termination status for next step\n",
    "            observations = observations_\n",
    "            terminations = terminations_\n",
    "\n",
    "            # Record episodic returns based on reward predictor rewards and environment rewards\n",
    "            for index, termination in enumerate(terminations):\n",
    "                if termination == 1:\n",
    "                    reward_predictor_reward_sum = reward_predictor_reward_sums[index]\n",
    "                    env_reward_sum = env_reward_sums[index]\n",
    "\n",
    "                    reward_predictor_episodic_returns.append(reward_predictor_reward_sum)\n",
    "\n",
    "                    log_info(f\"global_step={global_step}, episodic_reward_predictor_return={reward_predictor_reward_sum}\")\n",
    "                    log_info(f\"global_step={global_step}, episodic_env_return={env_reward_sum}\")\n",
    "\n",
    "                    # Writing reward stats to TensorBoard\n",
    "                    if pipeline_args.get('track_stats'):\n",
    "                        tensorboard_writer.add_scalar(\"ppo_agent/charts/episodic_reward_predictor_return\", reward_predictor_reward_sum, global_step)\n",
    "                        tensorboard_writer.add_scalar(\"ppo_agent/charts/episodic_env_return\", env_reward_sum, global_step)\n",
    "\n",
    "                    # Resetting rewards sum\n",
    "                    reward_predictor_reward_sums[index] = 0\n",
    "                    env_reward_sums[index] = 0\n",
    "\n",
    "        # Calculating current mean episodic return\n",
    "        current_mean_episodic_return = np.mean(reward_predictor_episodic_returns)\n",
    "        reward_predictor_episodic_returns.clear()\n",
    "        log_info(f\"Current Mean Episodic Return = {current_mean_episodic_return}\")\n",
    "\n",
    "        # Checking if the current mean is higher than previous highest mean \n",
    "        # and if the number of steps taken exceeds the model save threshold, and then saving the model\n",
    "        # if current_mean_episodic_return > best_average_return:\n",
    "        # Saving the model\n",
    "        model_save_path = f\"./models/rlhf_pipeline/training_run_{start_datetime_timestamp_str}/doom_ppo_agent/checkpoint_step_{global_step}\"\n",
    "        log_info(f\"Saving models to `{model_save_path}`...\")\n",
    "        agent.save_models(model_save_path)\n",
    "        log_info(f\"Successfully saved models to `{model_save_path}`!\")\n",
    "\n",
    "        # Saving new best average return and clearing returns arrays\n",
    "        best_average_return = current_mean_episodic_return\n",
    "            \n",
    "        # Calculating learning rate annealing coefficient for the agent\n",
    "        lr_anneal_coef = 1.0 - (update - 1.0) / num_updates\n",
    "\n",
    "        # Training the agent\n",
    "        agent_training_stats = agent.train(\n",
    "            replay_buffer=replay_buffer,\n",
    "            gamma=agent_args.get('gamma'),\n",
    "            enable_gae=agent_args.get('enable_gae'),\n",
    "            gae_lambda=agent_args.get('gae_lambda'),\n",
    "            clip_value_loss=agent_args.get('clip_value_loss'),\n",
    "            clip_coef=agent_args.get('clip_coef'),\n",
    "            max_norm_grad=agent_args.get('max_norm_grad'),\n",
    "            value_coef=agent_args.get('value_coef'),\n",
    "            entropy_coef=agent_args.get('entropy_coef'),\n",
    "            lr_anneal_coef=lr_anneal_coef,\n",
    "            target_kl=agent_args.get('target_kl'),\n",
    "            norm_adv=agent_args.get('norm_adv'),\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            num_training_epochs=agent_args.get('num_training_epochs'),\n",
    "        )\n",
    "        agent_training_iteration += 1\n",
    "        log_info(f\"SPS: {int(global_step / (time.time() - start_time))}\")\n",
    "\n",
    "        if pipeline_args.get('track_stats'):\n",
    "            tensorboard_writer.add_scalar(\"charts/learning_rate\", agent_training_stats.learning_rate, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/losses/value_loss\", agent_training_stats.value_loss, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/losses/policy_loss\", agent_training_stats.policy_loss, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/losses/entropy_loss\", agent_training_stats.entropy_loss, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/old_approx_kl\", agent_training_stats.old_approx_kl, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/approx_kl\", agent_training_stats.approx_kl, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/clip_fraction\", agent_training_stats.clip_fraction, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/explained_variance\", agent_training_stats.explained_variance, global_step)\n",
    "            tensorboard_writer.add_scalar(\"ppo_agent/charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "        if agent_training_iteration % pipeline_args.get('training_phase_human_feedback_interval') == 0:\n",
    "            # Preparing trajectories and videos for asking user for preference\n",
    "            show_loading_screen(\"Preparing trajectories for asking user for preference...\")\n",
    "            segments = replay_buffer.get_segments(segment_length=pipeline_args.get('num_frames_in_trajectory_video'))\n",
    "\n",
    "            trajectory_pairs = create_random_pairs(segments)\n",
    "\n",
    "            for trajectory_1, trajectory_2 in trajectory_pairs:\n",
    "                generate_video_from_doom_play_segments(trajectory_1, './temp/trajectory_1.mp4')\n",
    "                generate_video_from_doom_play_segments(trajectory_2, './temp/trajectory_2.mp4')\n",
    "                \n",
    "                # Asking user for their preference\n",
    "                user_preference = await ask_user_for_preference('./temp/trajectory_1.mp4', './temp/trajectory_2.mp4')\n",
    "                log_info(f\"user_preference = {user_preference}\")\n",
    "\n",
    "                # Training reward predictor based on the user preference\n",
    "                show_loading_screen(\"Pre-training reward predictor...\")\n",
    "                reward_predictor_training_stats = reward_predictor.train(trajectory_1, trajectory_2, user_preference, reward_predictor_args.get('num_training_epochs'))\n",
    "                log_info(f\"reward_predictor_training_loss={reward_predictor_training_stats['loss']}\")\n",
    "\n",
    "                if pipeline_args.get('track_stats'):\n",
    "                    # Writing loss value to tensorboard\n",
    "                    tensorboard_writer.add_scalar(\"ppo_agent/losses/loss\", reward_predictor_training_stats[\"loss\"], global_step)\n",
    "\n",
    "async def run_training_pipeline(envs: gym.vector.SyncVectorEnv, \n",
    "                                agent: DoomPpoAgent, \n",
    "                                reward_predictor: DoomHumanPreferenceRewardPredictor,\n",
    "                                pipeline_args: dict,\n",
    "                                agent_args: dict,\n",
    "                                reward_predictor_args: dict):\n",
    "    try:\n",
    "        if pipeline_args.get('enable_pre_training'):\n",
    "            await pre_train(envs, \n",
    "                            agent, \n",
    "                            reward_predictor, \n",
    "                            pipeline_args=pipeline_args,\n",
    "                            reward_predictor_args=reward_predictor_args)\n",
    "        await train(envs, \n",
    "                    agent, \n",
    "                    reward_predictor, \n",
    "                    pipeline_args,\n",
    "                    agent_args,\n",
    "                    reward_predictor_args)\n",
    "    except Exception as error:\n",
    "                # Catch the error and print the entire stack trace\n",
    "                traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Training Pipeline\n",
    "\n",
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_args = {\n",
    "    'env_cfg': 'envs/vizdoom/scenarios/basic.cfg',\n",
    "    'total_timesteps': 300000,\n",
    "    'render_env': True,\n",
    "    'model_save_threshold': 4000,\n",
    "    'enable_gpu': True,\n",
    "    'track_stats': True,\n",
    "    'num_envs': 8,\n",
    "    'env_replay_buffer_size': 256,\n",
    "    'enable_pre_training': False,\n",
    "    'num_pre_train_requests': 250,\n",
    "    'num_frames_in_trajectory_video': 64,\n",
    "    'training_phase_human_feedback_interval': 100\n",
    "}\n",
    "\n",
    "agent_args = {\n",
    "    'learning_rate': 0.0001,\n",
    "    'anneal_lr': True,\n",
    "    'enable_gae': True,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'num_mini_batches': 32,\n",
    "    'num_training_epochs': 10,\n",
    "    'norm_adv': True,\n",
    "    'clip_coef': 0.1,\n",
    "    'clip_value_loss': True,\n",
    "    'entropy_coef': 0.01,\n",
    "    'value_coef': 0.5,\n",
    "    'max_norm_grad': 0.5,\n",
    "    'target_kl': None\n",
    "}\n",
    "\n",
    "reward_predictor_args = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'hidden_layer_size': 64,\n",
    "    'num_training_epochs': 1,\n",
    "    'model_path': './models/rlhf_pipeline/training_run_2023_08_31_14_33_02/doom_reward_predictor/pre_training'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c204729f994b39bf4228a229a27f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\core.py:297: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_epochs=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haris\\anaconda3\\envs\\doom-rlhf\\lib\\site-packages\\gymnasium\\core.py:297: UserWarning: \u001b[33mWARN: env.raw_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.raw_observation_space` for environment variables or `env.get_attr('raw_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "%gui asyncio\n",
    "\n",
    "from utils.env import make_vizdoom_env\n",
    "\n",
    "# Displaying UI and showing only the loading screen\n",
    "clear_output()\n",
    "display(layout)\n",
    "hide_all_screens()\n",
    "\n",
    "# Initializing environments with only the first enviroment rendering\n",
    "num_envs = pipeline_args.get('num_envs')\n",
    "envs = gym.vector.SyncVectorEnv([ make_vizdoom_env('envs/vizdoom/scenarios/basic.cfg', render_mode=\"human\" if i == 0 else None) for i in range(num_envs)])\n",
    "\n",
    "# Setting up agent and reward predictor\n",
    "agent = DoomPpoAgent(envs.single_observation_space,\n",
    "                     envs.single_action_space,\n",
    "                     learning_rate=agent_args.get('learning_rate'),\n",
    "                     use_gpu=pipeline_args.get('enable_gpu'))\n",
    "reward_predictor = DoomHumanPreferenceRewardPredictor(envs.single_observation_space,\n",
    "                                                      hidden_layer_size=reward_predictor_args.get('hidden_layer_size'), \n",
    "                                                      learning_rate=reward_predictor_args.get('learning_rate'),\n",
    "                                                      use_gpu=pipeline_args.get('enable_gpu'))\n",
    "\n",
    "if reward_predictor_args.get('model_path'):\n",
    "    reward_predictor.load_models(reward_predictor_args.get('model_path'))\n",
    "\n",
    "# Starting training process\n",
    "event_loop = asyncio.get_event_loop()\n",
    "training_task = event_loop.create_task(run_training_pipeline(envs, \n",
    "                                                             agent, \n",
    "                                                             reward_predictor,\n",
    "                                                             pipeline_args=pipeline_args,\n",
    "                                                             agent_args=agent_args,\n",
    "                                                             reward_predictor_args=reward_predictor_args))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom-rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
