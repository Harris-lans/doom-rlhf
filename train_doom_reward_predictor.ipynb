{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Reward Predictor for Doom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.env import make_doom_env\n",
    "from agents.doom_ppo_agent import DoomPpoAgent\n",
    "from reward_predictors.cnn_reward_predictor import CnnRewardPredictor\n",
    "import gym\n",
    "from datetime import datetime\n",
    "from utils.memory import Memory\n",
    "from utils.time import current_timestamp_ms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Creating Environment\n",
    "num_envs = 4\n",
    "envs = gym.vector.SyncVectorEnv([ make_doom_env(level_config_path='vizdoom/scenarios/basic.cfg', render=False) for i in range(num_envs)])\n",
    "\n",
    "# Setting up agent\n",
    "agent = DoomPpoAgent(envs.single_observation_space, \n",
    "                     envs.single_action_space)\n",
    "                     #models_path='./models/doom_ppo_agent/training_run_2023_07_07_02_24_27/checkpoint_step_292864')\n",
    "reward_predictor = CnnRewardPredictor(envs.single_observation_space.shape, \n",
    "                                      1, \n",
    "                                      hidden_size=512, \n",
    "                                      learning_rate=0.0001)\n",
    "\n",
    "# Setting up agent training config\n",
    "global_step = 0\n",
    "start_datetime = datetime.now()\n",
    "start_time = start_datetime.timestamp()\n",
    "num_steps = 256\n",
    "num_mini_batches = 32\n",
    "num_training_epochs=10\n",
    "batch_size = int(num_envs * num_steps)\n",
    "mini_batch_size = batch_size // num_mini_batches\n",
    "total_timesteps = 50000\n",
    "num_updates = total_timesteps // batch_size\n",
    "memory = Memory(agent.device, num_steps, num_envs, envs.single_observation_space.shape, envs.single_action_space.shape)\n",
    "\n",
    "# Setting up debugging for Tensorboard\n",
    "tensorboard_writer = SummaryWriter(f\"logs/doom_basic_level/reward_predictor_training_{current_timestamp_ms()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=12, episodic_return=91.0\n",
      "global_step=32, episodic_return=67.0\n",
      "global_step=288, episodic_return=-240.0\n",
      "global_step=300, episodic_return=-365.0\n",
      "global_step=312, episodic_return=-370.0\n",
      "global_step=320, episodic_return=95.0\n",
      "global_step=328, episodic_return=95.0\n",
      "global_step=332, episodic_return=-370.0\n",
      "global_step=336, episodic_return=45.0\n",
      "global_step=344, episodic_return=95.0\n",
      "global_step=424, episodic_return=-14.0\n",
      "global_step=472, episodic_return=-75.0\n",
      "global_step=512, episodic_return=-4.0\n",
      "global_step=632, episodic_return=-340.0\n",
      "global_step=644, episodic_return=-365.0\n",
      "global_step=704, episodic_return=-124.0\n",
      "global_step=720, episodic_return=87.0\n",
      "global_step=728, episodic_return=95.0\n",
      "global_step=772, episodic_return=-365.0\n",
      "global_step=892, episodic_return=-207.0\n",
      "global_step=928, episodic_return=67.0\n",
      "global_step=944, episodic_return=87.0\n",
      "global_step=964, episodic_return=83.0\n",
      "global_step=996, episodic_return=71.0\n",
      "global_step=1004, episodic_return=95.0\n",
      "Avg Training Loss: 215.74954636784605\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_1024' created!\n",
      "Successfully saved models!\n",
      "global_step=1028, episodic_return=-375.0\n",
      "global_step=1036, episodic_return=95.0\n",
      "global_step=1044, episodic_return=95.0\n",
      "global_step=1072, episodic_return=-370.0\n",
      "global_step=1084, episodic_return=8.0\n",
      "global_step=1120, episodic_return=10.0\n",
      "global_step=1240, episodic_return=-223.0\n",
      "global_step=1248, episodic_return=95.0\n",
      "global_step=1260, episodic_return=-67.0\n",
      "global_step=1268, episodic_return=95.0\n",
      "global_step=1276, episodic_return=95.0\n",
      "global_step=1372, episodic_return=-355.0\n",
      "global_step=1384, episodic_return=-355.0\n",
      "global_step=1408, episodic_return=60.0\n",
      "global_step=1456, episodic_return=-155.0\n",
      "global_step=1488, episodic_return=13.0\n",
      "global_step=1552, episodic_return=-100.0\n",
      "global_step=1564, episodic_return=91.0\n",
      "global_step=1576, episodic_return=-365.0\n",
      "global_step=1584, episodic_return=95.0\n",
      "global_step=1592, episodic_return=95.0\n",
      "global_step=1628, episodic_return=-68.0\n",
      "global_step=1652, episodic_return=38.0\n",
      "global_step=1756, episodic_return=-365.0\n",
      "global_step=1764, episodic_return=95.0\n",
      "global_step=1776, episodic_return=-155.0\n",
      "global_step=1824, episodic_return=51.0\n",
      "global_step=1928, episodic_return=-385.0\n",
      "global_step=1944, episodic_return=87.0\n",
      "global_step=1952, episodic_return=-360.0\n",
      "Avg Training Loss: 212.27540639370636\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_2048' created!\n",
      "Successfully saved models!\n",
      "global_step=2064, episodic_return=-365.0\n",
      "global_step=2096, episodic_return=67.0\n",
      "global_step=2104, episodic_return=95.0\n",
      "global_step=2124, episodic_return=-355.0\n",
      "global_step=2172, episodic_return=25.0\n",
      "global_step=2180, episodic_return=35.0\n",
      "global_step=2192, episodic_return=91.0\n",
      "global_step=2196, episodic_return=-193.0\n",
      "global_step=2200, episodic_return=95.0\n",
      "global_step=2204, episodic_return=95.0\n",
      "global_step=2212, episodic_return=95.0\n",
      "global_step=2236, episodic_return=-255.0\n",
      "global_step=2244, episodic_return=95.0\n",
      "global_step=2264, episodic_return=83.0\n",
      "global_step=2272, episodic_return=95.0\n",
      "global_step=2288, episodic_return=-33.0\n",
      "global_step=2308, episodic_return=83.0\n",
      "global_step=2512, episodic_return=-360.0\n",
      "global_step=2528, episodic_return=87.0\n",
      "global_step=2536, episodic_return=79.0\n",
      "global_step=2564, episodic_return=62.0\n",
      "global_step=2572, episodic_return=-355.0\n",
      "global_step=2580, episodic_return=95.0\n",
      "global_step=2584, episodic_return=83.0\n",
      "global_step=2588, episodic_return=95.0\n",
      "global_step=2608, episodic_return=83.0\n",
      "global_step=2632, episodic_return=79.0\n",
      "global_step=2636, episodic_return=46.0\n",
      "global_step=2764, episodic_return=-55.0\n",
      "global_step=2776, episodic_return=91.0\n",
      "global_step=2812, episodic_return=-121.0\n",
      "global_step=2836, episodic_return=-375.0\n",
      "global_step=2856, episodic_return=76.0\n",
      "global_step=2872, episodic_return=29.0\n",
      "global_step=2904, episodic_return=51.0\n",
      "global_step=2908, episodic_return=-370.0\n",
      "global_step=2960, episodic_return=0.0\n",
      "global_step=2968, episodic_return=95.0\n",
      "global_step=2984, episodic_return=87.0\n",
      "global_step=2996, episodic_return=91.0\n",
      "Avg Training Loss: 225.19898547686898\n",
      "global_step=3076, episodic_return=-385.0\n",
      "global_step=3204, episodic_return=-375.0\n",
      "global_step=3208, episodic_return=-350.0\n",
      "global_step=3216, episodic_return=95.0\n",
      "global_step=3296, episodic_return=-370.0\n",
      "global_step=3304, episodic_return=95.0\n",
      "global_step=3376, episodic_return=-365.0\n",
      "global_step=3480, episodic_return=-28.0\n",
      "global_step=3504, episodic_return=-375.0\n",
      "global_step=3516, episodic_return=-365.0\n",
      "global_step=3548, episodic_return=71.0\n",
      "global_step=3604, episodic_return=-370.0\n",
      "global_step=3612, episodic_return=95.0\n",
      "global_step=3780, episodic_return=-355.0\n",
      "global_step=3804, episodic_return=-370.0\n",
      "global_step=3824, episodic_return=48.0\n",
      "global_step=3848, episodic_return=-370.0\n",
      "global_step=3860, episodic_return=91.0\n",
      "global_step=3896, episodic_return=27.0\n",
      "global_step=3900, episodic_return=58.0\n",
      "global_step=3912, episodic_return=-375.0\n",
      "global_step=3952, episodic_return=37.0\n",
      "global_step=3980, episodic_return=9.0\n",
      "global_step=4072, episodic_return=-47.0\n",
      "global_step=4096, episodic_return=-248.0\n",
      "Avg Training Loss: 94.50564730559199\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_4096' created!\n",
      "Successfully saved models!\n",
      "global_step=4140, episodic_return=-176.0\n",
      "global_step=4148, episodic_return=95.0\n",
      "global_step=4280, episodic_return=-375.0\n",
      "global_step=4396, episodic_return=-365.0\n",
      "global_step=4428, episodic_return=66.0\n",
      "global_step=4448, episodic_return=-370.0\n",
      "global_step=4508, episodic_return=3.0\n",
      "global_step=4560, episodic_return=34.0\n",
      "global_step=4580, episodic_return=-370.0\n",
      "global_step=4588, episodic_return=95.0\n",
      "global_step=4600, episodic_return=91.0\n",
      "global_step=4608, episodic_return=95.0\n",
      "global_step=4620, episodic_return=91.0\n",
      "global_step=4632, episodic_return=91.0\n",
      "global_step=4680, episodic_return=-246.0\n",
      "global_step=4692, episodic_return=91.0\n",
      "global_step=4704, episodic_return=91.0\n",
      "global_step=4720, episodic_return=87.0\n",
      "global_step=4748, episodic_return=-380.0\n",
      "global_step=4768, episodic_return=83.0\n",
      "global_step=4812, episodic_return=-200.0\n",
      "global_step=4980, episodic_return=-375.0\n",
      "global_step=4988, episodic_return=95.0\n",
      "global_step=5012, episodic_return=79.0\n",
      "global_step=5020, episodic_return=-370.0\n",
      "global_step=5024, episodic_return=91.0\n",
      "global_step=5068, episodic_return=-360.0\n",
      "global_step=5076, episodic_return=95.0\n",
      "global_step=5112, episodic_return=5.0\n",
      "global_step=5120, episodic_return=95.0\n",
      "Avg Training Loss: 71.75087774028725\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_5120' created!\n",
      "Successfully saved models!\n",
      "global_step=5144, episodic_return=64.0\n",
      "global_step=5252, episodic_return=-113.0\n",
      "global_step=5284, episodic_return=67.0\n",
      "global_step=5292, episodic_return=95.0\n",
      "global_step=5320, episodic_return=-365.0\n",
      "global_step=5400, episodic_return=13.0\n",
      "global_step=5420, episodic_return=-365.0\n",
      "global_step=5428, episodic_return=95.0\n",
      "global_step=5436, episodic_return=95.0\n",
      "global_step=5444, episodic_return=-360.0\n",
      "global_step=5468, episodic_return=67.0\n",
      "global_step=5524, episodic_return=-52.0\n",
      "global_step=5532, episodic_return=22.0\n",
      "global_step=5544, episodic_return=-201.0\n",
      "global_step=5556, episodic_return=72.0\n",
      "global_step=5572, episodic_return=87.0\n",
      "global_step=5608, episodic_return=62.0\n",
      "global_step=5696, episodic_return=-74.0\n",
      "global_step=5824, episodic_return=-385.0\n",
      "global_step=5832, episodic_return=95.0\n",
      "global_step=5856, episodic_return=-375.0\n",
      "global_step=5908, episodic_return=-380.0\n",
      "global_step=5996, episodic_return=-360.0\n",
      "global_step=6012, episodic_return=87.0\n",
      "global_step=6080, episodic_return=20.0\n",
      "global_step=6088, episodic_return=95.0\n",
      "global_step=6100, episodic_return=91.0\n",
      "global_step=6108, episodic_return=95.0\n",
      "global_step=6132, episodic_return=-365.0\n",
      "Avg Training Loss: 84.46560770504857\n",
      "global_step=6156, episodic_return=-375.0\n",
      "global_step=6208, episodic_return=-365.0\n",
      "global_step=6244, episodic_return=-6.0\n",
      "global_step=6328, episodic_return=-138.0\n",
      "global_step=6348, episodic_return=-64.0\n",
      "global_step=6356, episodic_return=95.0\n",
      "global_step=6372, episodic_return=87.0\n",
      "global_step=6408, episodic_return=-390.0\n",
      "global_step=6432, episodic_return=-26.0\n",
      "global_step=6440, episodic_return=71.0\n",
      "global_step=6448, episodic_return=95.0\n",
      "global_step=6456, episodic_return=95.0\n",
      "global_step=6544, episodic_return=-360.0\n",
      "global_step=6568, episodic_return=79.0\n",
      "global_step=6596, episodic_return=68.0\n",
      "global_step=6620, episodic_return=79.0\n",
      "global_step=6680, episodic_return=-200.0\n",
      "global_step=6756, episodic_return=-365.0\n",
      "global_step=6764, episodic_return=-61.0\n",
      "global_step=6768, episodic_return=91.0\n",
      "global_step=6772, episodic_return=95.0\n",
      "global_step=6808, episodic_return=52.0\n",
      "global_step=6920, episodic_return=-375.0\n",
      "global_step=6980, episodic_return=-370.0\n",
      "global_step=7008, episodic_return=75.0\n",
      "global_step=7072, episodic_return=-355.0\n",
      "global_step=7084, episodic_return=91.0\n",
      "global_step=7096, episodic_return=91.0\n",
      "global_step=7108, episodic_return=-370.0\n",
      "Avg Training Loss: 86.62306968117628\n",
      "global_step=7220, episodic_return=-365.0\n",
      "global_step=7232, episodic_return=91.0\n",
      "global_step=7236, episodic_return=-67.0\n",
      "global_step=7244, episodic_return=95.0\n",
      "global_step=7284, episodic_return=-108.0\n",
      "global_step=7308, episodic_return=-375.0\n",
      "global_step=7388, episodic_return=1.0\n",
      "global_step=7396, episodic_return=95.0\n",
      "global_step=7532, episodic_return=-365.0\n",
      "global_step=7540, episodic_return=95.0\n",
      "global_step=7544, episodic_return=-370.0\n",
      "global_step=7552, episodic_return=95.0\n",
      "global_step=7556, episodic_return=87.0\n",
      "global_step=7568, episodic_return=87.0\n",
      "global_step=7576, episodic_return=95.0\n",
      "global_step=7584, episodic_return=-365.0\n",
      "global_step=7652, episodic_return=-213.0\n",
      "global_step=7676, episodic_return=79.0\n",
      "global_step=7720, episodic_return=-78.0\n",
      "global_step=7812, episodic_return=-68.0\n",
      "global_step=7856, episodic_return=-370.0\n",
      "global_step=7864, episodic_return=95.0\n",
      "global_step=7872, episodic_return=95.0\n",
      "global_step=7884, episodic_return=-380.0\n",
      "global_step=7896, episodic_return=91.0\n",
      "global_step=7904, episodic_return=95.0\n",
      "global_step=7928, episodic_return=-37.0\n",
      "global_step=7936, episodic_return=95.0\n",
      "global_step=8020, episodic_return=-365.0\n",
      "global_step=8056, episodic_return=60.0\n",
      "global_step=8092, episodic_return=-88.0\n",
      "global_step=8104, episodic_return=91.0\n",
      "global_step=8136, episodic_return=67.0\n",
      "global_step=8156, episodic_return=83.0\n",
      "global_step=8172, episodic_return=-370.0\n",
      "global_step=8184, episodic_return=75.0\n",
      "Avg Training Loss: 95.45795167750839\n",
      "global_step=8204, episodic_return=-375.0\n",
      "global_step=8236, episodic_return=29.0\n",
      "global_step=8356, episodic_return=-375.0\n",
      "global_step=8380, episodic_return=-133.0\n",
      "global_step=8388, episodic_return=95.0\n",
      "global_step=8408, episodic_return=83.0\n",
      "global_step=8416, episodic_return=95.0\n",
      "global_step=8424, episodic_return=95.0\n",
      "global_step=8456, episodic_return=-12.0\n",
      "global_step=8464, episodic_return=95.0\n",
      "global_step=8504, episodic_return=-365.0\n",
      "global_step=8528, episodic_return=72.0\n",
      "global_step=8536, episodic_return=-355.0\n",
      "global_step=8544, episodic_return=-42.0\n",
      "global_step=8572, episodic_return=62.0\n",
      "global_step=8576, episodic_return=66.0\n",
      "global_step=8588, episodic_return=87.0\n",
      "global_step=8736, episodic_return=-229.0\n",
      "global_step=8752, episodic_return=-91.0\n",
      "global_step=8756, episodic_return=83.0\n",
      "global_step=8828, episodic_return=-370.0\n",
      "global_step=8876, episodic_return=-360.0\n",
      "global_step=8884, episodic_return=95.0\n",
      "global_step=8948, episodic_return=24.0\n",
      "global_step=8956, episodic_return=95.0\n",
      "global_step=8964, episodic_return=95.0\n",
      "global_step=9052, episodic_return=-365.0\n",
      "global_step=9056, episodic_return=-380.0\n",
      "global_step=9064, episodic_return=95.0\n",
      "global_step=9068, episodic_return=87.0\n",
      "global_step=9072, episodic_return=95.0\n",
      "global_step=9128, episodic_return=-360.0\n",
      "global_step=9184, episodic_return=-33.0\n",
      "global_step=9192, episodic_return=95.0\n",
      "Avg Training Loss: 75.66626569643995\n",
      "global_step=9224, episodic_return=67.0\n",
      "global_step=9264, episodic_return=-365.0\n",
      "global_step=9276, episodic_return=47.0\n",
      "global_step=9372, episodic_return=-370.0\n",
      "global_step=9376, episodic_return=-17.0\n",
      "global_step=9388, episodic_return=91.0\n",
      "global_step=9428, episodic_return=-355.0\n",
      "global_step=9496, episodic_return=-186.0\n",
      "global_step=9516, episodic_return=-6.0\n",
      "global_step=9536, episodic_return=-80.0\n",
      "global_step=9644, episodic_return=-236.0\n",
      "global_step=9796, episodic_return=-370.0\n",
      "global_step=9816, episodic_return=-365.0\n",
      "global_step=9836, episodic_return=-375.0\n",
      "global_step=9852, episodic_return=32.0\n",
      "global_step=9888, episodic_return=14.0\n",
      "global_step=9920, episodic_return=21.0\n",
      "global_step=9944, episodic_return=-365.0\n",
      "global_step=9952, episodic_return=66.0\n",
      "global_step=10016, episodic_return=-52.0\n",
      "global_step=10048, episodic_return=-13.0\n",
      "global_step=10060, episodic_return=91.0\n",
      "global_step=10088, episodic_return=75.0\n",
      "global_step=10136, episodic_return=-360.0\n",
      "Avg Training Loss: 77.79204146185839\n",
      "global_step=10244, episodic_return=-370.0\n",
      "global_step=10260, episodic_return=-183.0\n",
      "global_step=10268, episodic_return=95.0\n",
      "global_step=10388, episodic_return=-360.0\n",
      "global_step=10428, episodic_return=63.0\n",
      "global_step=10436, episodic_return=-360.0\n",
      "global_step=10448, episodic_return=91.0\n",
      "global_step=10544, episodic_return=-355.0\n",
      "global_step=10556, episodic_return=91.0\n",
      "global_step=10568, episodic_return=-370.0\n",
      "global_step=10612, episodic_return=42.0\n",
      "global_step=10672, episodic_return=34.0\n",
      "global_step=10728, episodic_return=-365.0\n",
      "global_step=10744, episodic_return=-262.0\n",
      "global_step=10752, episodic_return=8.0\n",
      "global_step=10868, episodic_return=-360.0\n",
      "global_step=10932, episodic_return=-117.0\n",
      "global_step=11000, episodic_return=30.0\n",
      "global_step=11028, episodic_return=-370.0\n",
      "global_step=11040, episodic_return=91.0\n",
      "global_step=11052, episodic_return=-380.0\n",
      "global_step=11088, episodic_return=62.0\n",
      "global_step=11168, episodic_return=-355.0\n",
      "global_step=11172, episodic_return=-54.0\n",
      "Avg Training Loss: 62.075059448050524\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_11264' created!\n",
      "Successfully saved models!\n",
      "global_step=11300, episodic_return=-365.0\n",
      "global_step=11388, episodic_return=-365.0\n",
      "global_step=11436, episodic_return=-57.0\n",
      "global_step=11468, episodic_return=-375.0\n",
      "global_step=11472, episodic_return=-370.0\n",
      "global_step=11480, episodic_return=95.0\n",
      "global_step=11516, episodic_return=-44.0\n",
      "global_step=11524, episodic_return=95.0\n",
      "global_step=11528, episodic_return=45.0\n",
      "global_step=11564, episodic_return=63.0\n",
      "global_step=11588, episodic_return=79.0\n",
      "global_step=11604, episodic_return=-51.0\n",
      "global_step=11612, episodic_return=72.0\n",
      "global_step=11644, episodic_return=67.0\n",
      "global_step=11656, episodic_return=91.0\n",
      "global_step=11664, episodic_return=95.0\n",
      "global_step=11736, episodic_return=-370.0\n",
      "global_step=11752, episodic_return=87.0\n",
      "global_step=11768, episodic_return=87.0\n",
      "global_step=11780, episodic_return=-205.0\n",
      "global_step=11788, episodic_return=83.0\n",
      "global_step=11800, episodic_return=91.0\n",
      "global_step=11872, episodic_return=-145.0\n",
      "global_step=11912, episodic_return=-375.0\n",
      "global_step=12016, episodic_return=-76.0\n",
      "global_step=12080, episodic_return=-355.0\n",
      "global_step=12088, episodic_return=95.0\n",
      "global_step=12120, episodic_return=71.0\n",
      "global_step=12124, episodic_return=-213.0\n",
      "global_step=12136, episodic_return=87.0\n",
      "global_step=12152, episodic_return=-187.0\n",
      "global_step=12160, episodic_return=95.0\n",
      "Avg Training Loss: 114.12067877741538\n",
      "global_step=12304, episodic_return=-71.0\n",
      "global_step=12316, episodic_return=-365.0\n",
      "global_step=12328, episodic_return=91.0\n",
      "global_step=12424, episodic_return=-365.0\n",
      "global_step=12428, episodic_return=-53.0\n",
      "global_step=12436, episodic_return=-370.0\n",
      "global_step=12444, episodic_return=83.0\n",
      "global_step=12492, episodic_return=32.0\n",
      "global_step=12500, episodic_return=37.0\n",
      "global_step=12508, episodic_return=95.0\n",
      "global_step=12512, episodic_return=91.0\n",
      "global_step=12516, episodic_return=95.0\n",
      "global_step=12528, episodic_return=91.0\n",
      "global_step=12588, episodic_return=38.0\n",
      "global_step=12628, episodic_return=-365.0\n",
      "global_step=12656, episodic_return=26.0\n",
      "global_step=12704, episodic_return=-129.0\n",
      "global_step=12720, episodic_return=87.0\n",
      "global_step=12724, episodic_return=-235.0\n",
      "global_step=12776, episodic_return=37.0\n",
      "global_step=12788, episodic_return=91.0\n",
      "global_step=12804, episodic_return=87.0\n",
      "global_step=12812, episodic_return=95.0\n",
      "global_step=12824, episodic_return=91.0\n",
      "global_step=12832, episodic_return=95.0\n",
      "global_step=12840, episodic_return=-159.0\n",
      "global_step=12868, episodic_return=63.0\n",
      "global_step=12872, episodic_return=71.0\n",
      "global_step=12880, episodic_return=91.0\n",
      "global_step=12948, episodic_return=-251.0\n",
      "global_step=13016, episodic_return=-76.0\n",
      "global_step=13024, episodic_return=-365.0\n",
      "global_step=13180, episodic_return=-375.0\n",
      "global_step=13224, episodic_return=-238.0\n",
      "global_step=13276, episodic_return=-8.0\n",
      "global_step=13288, episodic_return=91.0\n",
      "global_step=13300, episodic_return=91.0\n",
      "global_step=13308, episodic_return=95.0\n",
      "Avg Training Loss: 88.62660518605207\n",
      "global_step=13316, episodic_return=-370.0\n",
      "global_step=13324, episodic_return=-365.0\n",
      "global_step=13340, episodic_return=87.0\n",
      "global_step=13380, episodic_return=37.0\n",
      "global_step=13412, episodic_return=71.0\n",
      "global_step=13428, episodic_return=87.0\n",
      "global_step=13536, episodic_return=-22.0\n",
      "global_step=13616, episodic_return=-370.0\n",
      "global_step=13640, episodic_return=-365.0\n",
      "global_step=13796, episodic_return=-123.0\n",
      "global_step=13836, episodic_return=-370.0\n",
      "global_step=13940, episodic_return=-375.0\n",
      "global_step=13968, episodic_return=70.0\n",
      "global_step=14036, episodic_return=-144.0\n",
      "global_step=14096, episodic_return=-40.0\n",
      "global_step=14104, episodic_return=95.0\n",
      "global_step=14136, episodic_return=-365.0\n",
      "global_step=14152, episodic_return=87.0\n",
      "global_step=14160, episodic_return=-46.0\n",
      "global_step=14212, episodic_return=33.0\n",
      "Avg Training Loss: 62.01654236730883\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_14336' created!\n",
      "Successfully saved models!\n",
      "global_step=14364, episodic_return=-217.0\n",
      "global_step=14384, episodic_return=83.0\n",
      "global_step=14404, episodic_return=-360.0\n",
      "global_step=14460, episodic_return=-360.0\n",
      "global_step=14488, episodic_return=-237.0\n",
      "global_step=14496, episodic_return=95.0\n",
      "global_step=14520, episodic_return=-58.0\n",
      "global_step=14528, episodic_return=95.0\n",
      "global_step=14536, episodic_return=95.0\n",
      "global_step=14548, episodic_return=91.0\n",
      "global_step=14556, episodic_return=95.0\n",
      "global_step=14568, episodic_return=17.0\n",
      "global_step=14612, episodic_return=54.0\n",
      "global_step=14640, episodic_return=-180.0\n",
      "global_step=14648, episodic_return=95.0\n",
      "global_step=14708, episodic_return=33.0\n",
      "global_step=14740, episodic_return=64.0\n",
      "global_step=14760, episodic_return=-257.0\n",
      "global_step=14872, episodic_return=-190.0\n",
      "global_step=14904, episodic_return=-82.0\n",
      "global_step=14912, episodic_return=95.0\n",
      "global_step=14948, episodic_return=12.0\n",
      "global_step=15012, episodic_return=-12.0\n",
      "global_step=15040, episodic_return=-365.0\n",
      "global_step=15068, episodic_return=-32.0\n",
      "global_step=15092, episodic_return=8.0\n",
      "global_step=15116, episodic_return=79.0\n",
      "global_step=15128, episodic_return=91.0\n",
      "global_step=15160, episodic_return=67.0\n",
      "global_step=15188, episodic_return=21.0\n",
      "global_step=15212, episodic_return=-360.0\n",
      "global_step=15280, episodic_return=-149.0\n",
      "global_step=15300, episodic_return=-77.0\n",
      "global_step=15308, episodic_return=75.0\n",
      "global_step=15316, episodic_return=95.0\n",
      "global_step=15324, episodic_return=87.0\n",
      "Avg Training Loss: 115.010533833728\n",
      "global_step=15488, episodic_return=-360.0\n",
      "global_step=15504, episodic_return=-263.0\n",
      "global_step=15508, episodic_return=83.0\n",
      "global_step=15512, episodic_return=95.0\n",
      "global_step=15520, episodic_return=-151.0\n",
      "global_step=15524, episodic_return=-137.0\n",
      "global_step=15536, episodic_return=75.0\n",
      "global_step=15552, episodic_return=87.0\n",
      "global_step=15572, episodic_return=83.0\n",
      "global_step=15600, episodic_return=68.0\n",
      "global_step=15692, episodic_return=-117.0\n",
      "global_step=15820, episodic_return=-375.0\n",
      "global_step=15824, episodic_return=-375.0\n",
      "global_step=15828, episodic_return=95.0\n",
      "global_step=15852, episodic_return=72.0\n",
      "global_step=15884, episodic_return=71.0\n",
      "global_step=15900, episodic_return=-370.0\n",
      "global_step=15904, episodic_return=76.0\n",
      "global_step=15992, episodic_return=-370.0\n",
      "global_step=16044, episodic_return=42.0\n",
      "global_step=16068, episodic_return=72.0\n",
      "global_step=16124, episodic_return=-355.0\n",
      "global_step=16136, episodic_return=18.0\n",
      "global_step=16160, episodic_return=72.0\n",
      "global_step=16172, episodic_return=91.0\n",
      "global_step=16200, episodic_return=-375.0\n",
      "global_step=16204, episodic_return=-365.0\n",
      "global_step=16312, episodic_return=-119.0\n",
      "Avg Training Loss: 59.31338406850716\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_16384' created!\n",
      "Successfully saved models!\n",
      "global_step=16448, episodic_return=-181.0\n",
      "global_step=16460, episodic_return=-82.0\n",
      "global_step=16472, episodic_return=-385.0\n",
      "global_step=16500, episodic_return=-375.0\n",
      "global_step=16508, episodic_return=95.0\n",
      "global_step=16524, episodic_return=87.0\n",
      "global_step=16536, episodic_return=91.0\n",
      "global_step=16564, episodic_return=-9.0\n",
      "global_step=16648, episodic_return=-130.0\n",
      "global_step=16656, episodic_return=95.0\n",
      "global_step=16664, episodic_return=95.0\n",
      "global_step=16672, episodic_return=-25.0\n",
      "global_step=16684, episodic_return=91.0\n",
      "global_step=16748, episodic_return=-365.0\n",
      "global_step=16784, episodic_return=67.0\n",
      "global_step=16836, episodic_return=-380.0\n",
      "global_step=16840, episodic_return=-118.0\n",
      "global_step=16844, episodic_return=95.0\n",
      "global_step=16848, episodic_return=95.0\n",
      "global_step=16872, episodic_return=75.0\n",
      "global_step=16884, episodic_return=62.0\n",
      "global_step=16892, episodic_return=95.0\n",
      "global_step=16984, episodic_return=-355.0\n",
      "global_step=16992, episodic_return=95.0\n",
      "global_step=17008, episodic_return=79.0\n",
      "global_step=17032, episodic_return=59.0\n",
      "global_step=17044, episodic_return=91.0\n",
      "global_step=17084, episodic_return=-365.0\n",
      "global_step=17100, episodic_return=87.0\n",
      "global_step=17128, episodic_return=4.0\n",
      "global_step=17136, episodic_return=95.0\n",
      "global_step=17192, episodic_return=-370.0\n",
      "global_step=17196, episodic_return=38.0\n",
      "global_step=17212, episodic_return=83.0\n",
      "global_step=17280, episodic_return=-112.0\n",
      "global_step=17344, episodic_return=-380.0\n",
      "global_step=17380, episodic_return=-17.0\n",
      "global_step=17400, episodic_return=83.0\n",
      "Avg Training Loss: 111.51644245705756\n",
      "global_step=17496, episodic_return=-360.0\n",
      "global_step=17512, episodic_return=-375.0\n",
      "global_step=17644, episodic_return=-370.0\n",
      "global_step=17700, episodic_return=-365.0\n",
      "global_step=17792, episodic_return=-237.0\n",
      "global_step=17796, episodic_return=-370.0\n",
      "global_step=17864, episodic_return=20.0\n",
      "global_step=17928, episodic_return=-182.0\n",
      "global_step=17944, episodic_return=-380.0\n",
      "global_step=17948, episodic_return=-93.0\n",
      "global_step=17964, episodic_return=87.0\n",
      "global_step=18072, episodic_return=-66.0\n",
      "global_step=18088, episodic_return=87.0\n",
      "global_step=18140, episodic_return=39.0\n",
      "global_step=18152, episodic_return=91.0\n",
      "global_step=18164, episodic_return=-385.0\n",
      "global_step=18244, episodic_return=-370.0\n",
      "global_step=18264, episodic_return=-365.0\n",
      "global_step=18320, episodic_return=-83.0\n",
      "global_step=18328, episodic_return=95.0\n",
      "global_step=18372, episodic_return=54.0\n",
      "global_step=18420, episodic_return=-229.0\n",
      "Avg Training Loss: 41.904474301510845\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_18432' created!\n",
      "Successfully saved models!\n",
      "global_step=18564, episodic_return=-365.0\n",
      "global_step=18672, episodic_return=-370.0\n",
      "global_step=18720, episodic_return=-370.0\n",
      "global_step=18728, episodic_return=95.0\n",
      "global_step=18780, episodic_return=-163.0\n",
      "global_step=18828, episodic_return=-83.0\n",
      "global_step=18832, episodic_return=-210.0\n",
      "global_step=18844, episodic_return=87.0\n",
      "global_step=18868, episodic_return=79.0\n",
      "global_step=18880, episodic_return=50.0\n",
      "global_step=18940, episodic_return=-99.0\n",
      "global_step=18944, episodic_return=24.0\n",
      "global_step=19028, episodic_return=-375.0\n",
      "global_step=19072, episodic_return=-130.0\n",
      "global_step=19204, episodic_return=-206.0\n",
      "global_step=19216, episodic_return=91.0\n",
      "global_step=19224, episodic_return=95.0\n",
      "global_step=19232, episodic_return=95.0\n",
      "global_step=19244, episodic_return=-355.0\n",
      "global_step=19304, episodic_return=38.0\n",
      "global_step=19328, episodic_return=-370.0\n",
      "global_step=19372, episodic_return=-365.0\n",
      "global_step=19380, episodic_return=95.0\n",
      "global_step=19404, episodic_return=-23.0\n",
      "global_step=19452, episodic_return=-174.0\n",
      "global_step=19456, episodic_return=18.0\n",
      "Avg Training Loss: 70.19955449986719\n",
      "global_step=19464, episodic_return=33.0\n",
      "global_step=19476, episodic_return=91.0\n",
      "global_step=19488, episodic_return=-87.0\n",
      "global_step=19492, episodic_return=67.0\n",
      "global_step=19500, episodic_return=95.0\n",
      "global_step=19516, episodic_return=87.0\n",
      "global_step=19524, episodic_return=95.0\n",
      "global_step=19588, episodic_return=29.0\n",
      "global_step=19616, episodic_return=75.0\n",
      "global_step=19652, episodic_return=-132.0\n",
      "global_step=19668, episodic_return=87.0\n",
      "global_step=19676, episodic_return=95.0\n",
      "global_step=19680, episodic_return=29.0\n",
      "global_step=19688, episodic_return=95.0\n",
      "global_step=19708, episodic_return=71.0\n",
      "global_step=19724, episodic_return=87.0\n",
      "global_step=19732, episodic_return=95.0\n",
      "global_step=19740, episodic_return=95.0\n",
      "global_step=19748, episodic_return=95.0\n",
      "global_step=19788, episodic_return=-360.0\n",
      "global_step=19800, episodic_return=91.0\n",
      "global_step=19844, episodic_return=54.0\n",
      "global_step=19848, episodic_return=-23.0\n",
      "global_step=19932, episodic_return=0.0\n",
      "global_step=19964, episodic_return=-233.0\n",
      "global_step=19972, episodic_return=95.0\n",
      "global_step=19996, episodic_return=79.0\n",
      "global_step=20020, episodic_return=-98.0\n",
      "global_step=20024, episodic_return=-380.0\n",
      "global_step=20036, episodic_return=87.0\n",
      "global_step=20072, episodic_return=44.0\n",
      "global_step=20076, episodic_return=59.0\n",
      "global_step=20200, episodic_return=-51.0\n",
      "global_step=20208, episodic_return=95.0\n",
      "global_step=20216, episodic_return=95.0\n",
      "global_step=20232, episodic_return=-375.0\n",
      "global_step=20240, episodic_return=95.0\n",
      "global_step=20296, episodic_return=-365.0\n",
      "global_step=20360, episodic_return=34.0\n",
      "global_step=20368, episodic_return=-263.0\n",
      "global_step=20376, episodic_return=95.0\n",
      "global_step=20388, episodic_return=91.0\n",
      "global_step=20400, episodic_return=91.0\n",
      "global_step=20428, episodic_return=-125.0\n",
      "global_step=20448, episodic_return=51.0\n",
      "global_step=20464, episodic_return=-28.0\n",
      "global_step=20472, episodic_return=95.0\n",
      "Avg Training Loss: 119.7426027656411\n",
      "global_step=20484, episodic_return=91.0\n",
      "global_step=20492, episodic_return=95.0\n",
      "global_step=20516, episodic_return=-360.0\n",
      "global_step=20528, episodic_return=60.0\n",
      "global_step=20532, episodic_return=87.0\n",
      "global_step=20540, episodic_return=91.0\n",
      "global_step=20556, episodic_return=87.0\n",
      "global_step=20728, episodic_return=-370.0\n",
      "global_step=20748, episodic_return=-370.0\n",
      "global_step=20772, episodic_return=-167.0\n",
      "global_step=20788, episodic_return=87.0\n",
      "global_step=20832, episodic_return=-370.0\n",
      "global_step=20916, episodic_return=-105.0\n",
      "global_step=20928, episodic_return=91.0\n",
      "global_step=21028, episodic_return=-365.0\n",
      "global_step=21072, episodic_return=-71.0\n",
      "global_step=21088, episodic_return=-360.0\n",
      "global_step=21096, episodic_return=-221.0\n",
      "global_step=21100, episodic_return=91.0\n",
      "global_step=21152, episodic_return=-41.0\n",
      "global_step=21208, episodic_return=-25.0\n",
      "global_step=21284, episodic_return=-127.0\n",
      "global_step=21292, episodic_return=95.0\n",
      "global_step=21304, episodic_return=91.0\n",
      "global_step=21332, episodic_return=-119.0\n",
      "global_step=21352, episodic_return=83.0\n",
      "global_step=21364, episodic_return=91.0\n",
      "global_step=21372, episodic_return=-375.0\n",
      "global_step=21452, episodic_return=-5.0\n",
      "global_step=21500, episodic_return=43.0\n",
      "Avg Training Loss: 60.02785911997307\n",
      "global_step=21508, episodic_return=-375.0\n",
      "global_step=21532, episodic_return=79.0\n",
      "global_step=21560, episodic_return=75.0\n",
      "global_step=21572, episodic_return=91.0\n",
      "global_step=21604, episodic_return=-385.0\n",
      "global_step=21672, episodic_return=-370.0\n",
      "global_step=21680, episodic_return=95.0\n",
      "global_step=21688, episodic_return=95.0\n",
      "global_step=21692, episodic_return=-41.0\n",
      "global_step=21696, episodic_return=95.0\n",
      "global_step=21796, episodic_return=-262.0\n",
      "global_step=21848, episodic_return=46.0\n",
      "global_step=21868, episodic_return=83.0\n",
      "global_step=21876, episodic_return=95.0\n",
      "global_step=21904, episodic_return=-375.0\n",
      "global_step=21912, episodic_return=95.0\n",
      "global_step=21964, episodic_return=-7.0\n",
      "global_step=21984, episodic_return=21.0\n",
      "global_step=21992, episodic_return=-360.0\n",
      "global_step=21996, episodic_return=-360.0\n",
      "global_step=22020, episodic_return=79.0\n",
      "global_step=22040, episodic_return=51.0\n",
      "global_step=22052, episodic_return=26.0\n",
      "global_step=22056, episodic_return=62.0\n",
      "global_step=22108, episodic_return=44.0\n",
      "global_step=22264, episodic_return=-365.0\n",
      "global_step=22320, episodic_return=-153.0\n",
      "global_step=22340, episodic_return=-370.0\n",
      "global_step=22352, episodic_return=-380.0\n",
      "global_step=22384, episodic_return=66.0\n",
      "global_step=22388, episodic_return=45.0\n",
      "Avg Training Loss: 29.672011300024224\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_22528' created!\n",
      "Successfully saved models!\n",
      "global_step=22544, episodic_return=-78.0\n",
      "global_step=22556, episodic_return=91.0\n",
      "global_step=22564, episodic_return=-365.0\n",
      "global_step=22592, episodic_return=63.0\n",
      "global_step=22620, episodic_return=-375.0\n",
      "global_step=22628, episodic_return=95.0\n",
      "global_step=22684, episodic_return=-375.0\n",
      "global_step=22692, episodic_return=24.0\n",
      "global_step=22780, episodic_return=1.0\n",
      "global_step=22860, episodic_return=3.0\n",
      "global_step=22864, episodic_return=-365.0\n",
      "global_step=22872, episodic_return=95.0\n",
      "global_step=22892, episodic_return=-375.0\n",
      "global_step=22900, episodic_return=95.0\n",
      "global_step=22920, episodic_return=-173.0\n",
      "global_step=23016, episodic_return=-13.0\n",
      "global_step=23032, episodic_return=87.0\n",
      "global_step=23052, episodic_return=-111.0\n",
      "global_step=23084, episodic_return=-117.0\n",
      "global_step=23116, episodic_return=66.0\n",
      "global_step=23124, episodic_return=95.0\n",
      "global_step=23160, episodic_return=-375.0\n",
      "global_step=23180, episodic_return=76.0\n",
      "global_step=23184, episodic_return=79.0\n",
      "global_step=23268, episodic_return=4.0\n",
      "global_step=23272, episodic_return=-182.0\n",
      "global_step=23312, episodic_return=-54.0\n",
      "global_step=23352, episodic_return=-375.0\n",
      "global_step=23360, episodic_return=95.0\n",
      "global_step=23380, episodic_return=83.0\n",
      "global_step=23528, episodic_return=-158.0\n",
      "global_step=23536, episodic_return=95.0\n",
      "global_step=23548, episodic_return=91.0\n",
      "Avg Training Loss: 65.62072069024316\n",
      "global_step=23568, episodic_return=-380.0\n",
      "global_step=23572, episodic_return=72.0\n",
      "global_step=23592, episodic_return=79.0\n",
      "global_step=23660, episodic_return=-4.0\n",
      "global_step=23668, episodic_return=95.0\n",
      "global_step=23680, episodic_return=-355.0\n",
      "global_step=23696, episodic_return=87.0\n",
      "global_step=23736, episodic_return=-73.0\n",
      "global_step=23872, episodic_return=-375.0\n",
      "global_step=23876, episodic_return=-74.0\n",
      "global_step=23888, episodic_return=91.0\n",
      "global_step=23928, episodic_return=37.0\n",
      "global_step=23960, episodic_return=-206.0\n",
      "global_step=23968, episodic_return=-355.0\n",
      "global_step=24188, episodic_return=-370.0\n",
      "global_step=24196, episodic_return=95.0\n",
      "global_step=24204, episodic_return=95.0\n",
      "global_step=24216, episodic_return=91.0\n",
      "global_step=24224, episodic_return=95.0\n",
      "global_step=24228, episodic_return=-375.0\n",
      "global_step=24240, episodic_return=87.0\n",
      "global_step=24260, episodic_return=-360.0\n",
      "global_step=24268, episodic_return=-360.0\n",
      "global_step=24280, episodic_return=91.0\n",
      "global_step=24316, episodic_return=60.0\n",
      "global_step=24348, episodic_return=0.0\n",
      "global_step=24452, episodic_return=-165.0\n",
      "global_step=24500, episodic_return=-123.0\n",
      "global_step=24516, episodic_return=87.0\n",
      "global_step=24532, episodic_return=87.0\n",
      "global_step=24540, episodic_return=-375.0\n",
      "Avg Training Loss: 58.18053813802294\n",
      "global_step=24648, episodic_return=-375.0\n",
      "global_step=24728, episodic_return=-135.0\n",
      "global_step=24736, episodic_return=95.0\n",
      "global_step=24752, episodic_return=-370.0\n",
      "global_step=24760, episodic_return=95.0\n",
      "global_step=24800, episodic_return=-84.0\n",
      "global_step=24840, episodic_return=-365.0\n",
      "global_step=24888, episodic_return=55.0\n",
      "global_step=25016, episodic_return=-226.0\n",
      "global_step=25092, episodic_return=-204.0\n",
      "global_step=25100, episodic_return=-370.0\n",
      "global_step=25108, episodic_return=-162.0\n",
      "global_step=25116, episodic_return=87.0\n",
      "global_step=25176, episodic_return=25.0\n",
      "global_step=25184, episodic_return=95.0\n",
      "global_step=25192, episodic_return=95.0\n",
      "global_step=25232, episodic_return=-69.0\n",
      "global_step=25240, episodic_return=-46.0\n",
      "global_step=25316, episodic_return=-380.0\n",
      "global_step=25328, episodic_return=91.0\n",
      "global_step=25336, episodic_return=-60.0\n",
      "global_step=25344, episodic_return=95.0\n",
      "global_step=25348, episodic_return=91.0\n",
      "global_step=25380, episodic_return=27.0\n",
      "global_step=25448, episodic_return=19.0\n",
      "global_step=25452, episodic_return=-6.0\n",
      "global_step=25540, episodic_return=-380.0\n",
      "global_step=25548, episodic_return=95.0\n",
      "global_step=25600, episodic_return=42.0\n",
      "Avg Training Loss: 67.57356127107323\n",
      "global_step=25624, episodic_return=79.0\n",
      "global_step=25644, episodic_return=-380.0\n",
      "global_step=25696, episodic_return=15.0\n",
      "global_step=25704, episodic_return=95.0\n",
      "global_step=25740, episodic_return=-240.0\n",
      "global_step=25748, episodic_return=-370.0\n",
      "global_step=25784, episodic_return=60.0\n",
      "global_step=25908, episodic_return=-141.0\n",
      "global_step=25944, episodic_return=-370.0\n",
      "global_step=25976, episodic_return=-124.0\n",
      "global_step=26016, episodic_return=-235.0\n",
      "global_step=26088, episodic_return=10.0\n",
      "global_step=26124, episodic_return=-82.0\n",
      "global_step=26140, episodic_return=-135.0\n",
      "global_step=26152, episodic_return=91.0\n",
      "global_step=26160, episodic_return=95.0\n",
      "global_step=26208, episodic_return=-350.0\n",
      "global_step=26260, episodic_return=41.0\n",
      "global_step=26292, episodic_return=-136.0\n",
      "global_step=26352, episodic_return=34.0\n",
      "global_step=26372, episodic_return=83.0\n",
      "global_step=26376, episodic_return=-165.0\n",
      "global_step=26396, episodic_return=-58.0\n",
      "global_step=26424, episodic_return=-370.0\n",
      "global_step=26448, episodic_return=72.0\n",
      "global_step=26492, episodic_return=-37.0\n",
      "global_step=26508, episodic_return=87.0\n",
      "global_step=26556, episodic_return=50.0\n",
      "global_step=26624, episodic_return=20.0\n",
      "Avg Training Loss: 45.83327264618174\n",
      "global_step=26672, episodic_return=-375.0\n",
      "global_step=26684, episodic_return=91.0\n",
      "global_step=26696, episodic_return=-370.0\n",
      "global_step=26712, episodic_return=0.0\n",
      "global_step=26720, episodic_return=95.0\n",
      "global_step=26744, episodic_return=79.0\n",
      "global_step=26748, episodic_return=-365.0\n",
      "global_step=26760, episodic_return=87.0\n",
      "global_step=26776, episodic_return=87.0\n",
      "global_step=26820, episodic_return=-51.0\n",
      "global_step=26844, episodic_return=20.0\n",
      "global_step=26852, episodic_return=95.0\n",
      "global_step=26900, episodic_return=-158.0\n",
      "global_step=26956, episodic_return=-149.0\n",
      "global_step=26976, episodic_return=-88.0\n",
      "global_step=26984, episodic_return=95.0\n",
      "global_step=26992, episodic_return=95.0\n",
      "global_step=27056, episodic_return=-6.0\n",
      "global_step=27152, episodic_return=-375.0\n",
      "global_step=27160, episodic_return=-22.0\n",
      "global_step=27188, episodic_return=75.0\n",
      "global_step=27200, episodic_return=-375.0\n",
      "global_step=27208, episodic_return=37.0\n",
      "global_step=27216, episodic_return=75.0\n",
      "global_step=27228, episodic_return=-182.0\n",
      "global_step=27236, episodic_return=83.0\n",
      "global_step=27276, episodic_return=22.0\n",
      "global_step=27292, episodic_return=29.0\n",
      "global_step=27304, episodic_return=20.0\n",
      "global_step=27312, episodic_return=76.0\n",
      "global_step=27320, episodic_return=95.0\n",
      "global_step=27388, episodic_return=20.0\n",
      "global_step=27408, episodic_return=-26.0\n",
      "global_step=27416, episodic_return=95.0\n",
      "global_step=27460, episodic_return=16.0\n",
      "global_step=27468, episodic_return=95.0\n",
      "global_step=27508, episodic_return=-360.0\n",
      "global_step=27516, episodic_return=95.0\n",
      "global_step=27528, episodic_return=91.0\n",
      "global_step=27536, episodic_return=95.0\n",
      "global_step=27576, episodic_return=-375.0\n",
      "global_step=27604, episodic_return=70.0\n",
      "global_step=27612, episodic_return=12.0\n",
      "global_step=27632, episodic_return=83.0\n",
      "Avg Training Loss: 118.50718231501423\n",
      "global_step=27768, episodic_return=-370.0\n",
      "global_step=27816, episodic_return=-365.0\n",
      "global_step=27840, episodic_return=79.0\n",
      "global_step=27904, episodic_return=-375.0\n",
      "global_step=27916, episodic_return=91.0\n",
      "global_step=27932, episodic_return=-249.0\n",
      "global_step=27952, episodic_return=76.0\n",
      "global_step=27960, episodic_return=95.0\n",
      "global_step=27968, episodic_return=95.0\n",
      "global_step=27996, episodic_return=-78.0\n",
      "global_step=28004, episodic_return=95.0\n",
      "global_step=28068, episodic_return=-375.0\n",
      "global_step=28112, episodic_return=54.0\n",
      "global_step=28200, episodic_return=-256.0\n",
      "global_step=28208, episodic_return=95.0\n",
      "global_step=28236, episodic_return=70.0\n",
      "global_step=28268, episodic_return=-375.0\n",
      "global_step=28304, episodic_return=-360.0\n",
      "global_step=28320, episodic_return=87.0\n",
      "global_step=28328, episodic_return=95.0\n",
      "global_step=28348, episodic_return=83.0\n",
      "global_step=28412, episodic_return=-360.0\n",
      "global_step=28416, episodic_return=21.0\n",
      "global_step=28424, episodic_return=95.0\n",
      "global_step=28456, episodic_return=71.0\n",
      "global_step=28464, episodic_return=-176.0\n",
      "global_step=28468, episodic_return=91.0\n",
      "global_step=28480, episodic_return=91.0\n",
      "global_step=28488, episodic_return=95.0\n",
      "global_step=28568, episodic_return=-380.0\n",
      "global_step=28584, episodic_return=-14.0\n",
      "global_step=28660, episodic_return=-15.0\n",
      "Avg Training Loss: 44.87288603179712\n",
      "global_step=28764, episodic_return=-375.0\n",
      "global_step=28768, episodic_return=-370.0\n",
      "global_step=28776, episodic_return=95.0\n",
      "global_step=28784, episodic_return=83.0\n",
      "global_step=28884, episodic_return=-375.0\n",
      "global_step=28896, episodic_return=91.0\n",
      "global_step=28960, episodic_return=-365.0\n",
      "global_step=29044, episodic_return=-227.0\n",
      "global_step=29080, episodic_return=-259.0\n",
      "global_step=29168, episodic_return=-150.0\n",
      "global_step=29188, episodic_return=76.0\n",
      "global_step=29196, episodic_return=-375.0\n",
      "global_step=29204, episodic_return=-41.0\n",
      "global_step=29248, episodic_return=33.0\n",
      "global_step=29256, episodic_return=95.0\n",
      "global_step=29300, episodic_return=-14.0\n",
      "global_step=29320, episodic_return=29.0\n",
      "global_step=29344, episodic_return=-365.0\n",
      "global_step=29448, episodic_return=-22.0\n",
      "global_step=29456, episodic_return=95.0\n",
      "global_step=29496, episodic_return=-370.0\n",
      "global_step=29600, episodic_return=-365.0\n",
      "global_step=29620, episodic_return=-380.0\n",
      "global_step=29632, episodic_return=-108.0\n",
      "global_step=29692, episodic_return=33.0\n",
      "Avg Training Loss: 74.99935839781392\n",
      "global_step=29704, episodic_return=91.0\n",
      "global_step=29748, episodic_return=52.0\n",
      "global_step=29796, episodic_return=-365.0\n",
      "global_step=29808, episodic_return=-150.0\n",
      "global_step=29856, episodic_return=50.0\n",
      "global_step=29876, episodic_return=13.0\n",
      "global_step=29884, episodic_return=75.0\n",
      "global_step=29892, episodic_return=87.0\n",
      "global_step=29896, episodic_return=-65.0\n",
      "global_step=29900, episodic_return=95.0\n",
      "global_step=29908, episodic_return=91.0\n",
      "global_step=29916, episodic_return=95.0\n",
      "global_step=29920, episodic_return=91.0\n",
      "global_step=29928, episodic_return=95.0\n",
      "global_step=29936, episodic_return=95.0\n",
      "global_step=29952, episodic_return=47.0\n",
      "global_step=30112, episodic_return=-134.0\n",
      "global_step=30140, episodic_return=68.0\n",
      "global_step=30148, episodic_return=95.0\n",
      "global_step=30156, episodic_return=95.0\n",
      "global_step=30180, episodic_return=-176.0\n",
      "global_step=30216, episodic_return=-365.0\n",
      "global_step=30252, episodic_return=-370.0\n",
      "global_step=30308, episodic_return=-45.0\n",
      "global_step=30328, episodic_return=83.0\n",
      "global_step=30336, episodic_return=95.0\n",
      "global_step=30368, episodic_return=-159.0\n",
      "global_step=30428, episodic_return=33.0\n",
      "global_step=30460, episodic_return=66.0\n",
      "global_step=30468, episodic_return=95.0\n",
      "global_step=30476, episodic_return=95.0\n",
      "global_step=30500, episodic_return=72.0\n",
      "global_step=30516, episodic_return=-370.0\n",
      "global_step=30540, episodic_return=-245.0\n",
      "global_step=30552, episodic_return=91.0\n",
      "global_step=30560, episodic_return=59.0\n",
      "global_step=30572, episodic_return=83.0\n",
      "global_step=30636, episodic_return=29.0\n",
      "Avg Training Loss: 85.15664128207663\n",
      "global_step=30768, episodic_return=-150.0\n",
      "global_step=30800, episodic_return=-375.0\n",
      "global_step=30816, episodic_return=-123.0\n",
      "global_step=30824, episodic_return=72.0\n",
      "global_step=30832, episodic_return=95.0\n",
      "global_step=30840, episodic_return=95.0\n",
      "global_step=30884, episodic_return=59.0\n",
      "global_step=30936, episodic_return=-375.0\n",
      "global_step=31068, episodic_return=-365.0\n",
      "global_step=31080, episodic_return=91.0\n",
      "global_step=31096, episodic_return=87.0\n",
      "global_step=31116, episodic_return=-375.0\n",
      "global_step=31144, episodic_return=75.0\n",
      "global_step=31156, episodic_return=-166.0\n",
      "global_step=31164, episodic_return=26.0\n",
      "global_step=31168, episodic_return=91.0\n",
      "global_step=31180, episodic_return=91.0\n",
      "global_step=31184, episodic_return=83.0\n",
      "global_step=31192, episodic_return=95.0\n",
      "global_step=31204, episodic_return=91.0\n",
      "global_step=31216, episodic_return=62.0\n",
      "global_step=31220, episodic_return=87.0\n",
      "global_step=31308, episodic_return=1.0\n",
      "global_step=31324, episodic_return=87.0\n",
      "global_step=31332, episodic_return=95.0\n",
      "global_step=31344, episodic_return=91.0\n",
      "global_step=31368, episodic_return=79.0\n",
      "global_step=31384, episodic_return=87.0\n",
      "global_step=31432, episodic_return=-245.0\n",
      "global_step=31484, episodic_return=-360.0\n",
      "global_step=31520, episodic_return=-355.0\n",
      "global_step=31560, episodic_return=52.0\n",
      "global_step=31624, episodic_return=-124.0\n",
      "global_step=31648, episodic_return=79.0\n",
      "global_step=31684, episodic_return=-375.0\n",
      "global_step=31724, episodic_return=12.0\n",
      "Avg Training Loss: 23.968336263670608\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_31744' created!\n",
      "Successfully saved models!\n",
      "global_step=31776, episodic_return=-9.0\n",
      "global_step=31784, episodic_return=-375.0\n",
      "global_step=31792, episodic_return=95.0\n",
      "global_step=31860, episodic_return=-350.0\n",
      "global_step=31868, episodic_return=95.0\n",
      "global_step=31936, episodic_return=18.0\n",
      "global_step=31944, episodic_return=95.0\n",
      "global_step=31968, episodic_return=79.0\n",
      "global_step=31976, episodic_return=95.0\n",
      "global_step=32012, episodic_return=57.0\n",
      "global_step=32024, episodic_return=-375.0\n",
      "global_step=32052, episodic_return=75.0\n",
      "global_step=32076, episodic_return=-350.0\n",
      "global_step=32084, episodic_return=71.0\n",
      "global_step=32092, episodic_return=-365.0\n",
      "global_step=32128, episodic_return=62.0\n",
      "global_step=32200, episodic_return=-120.0\n",
      "global_step=32364, episodic_return=-239.0\n",
      "global_step=32376, episodic_return=-365.0\n",
      "global_step=32428, episodic_return=-365.0\n",
      "global_step=32436, episodic_return=95.0\n",
      "global_step=32484, episodic_return=-30.0\n",
      "global_step=32500, episodic_return=-355.0\n",
      "global_step=32520, episodic_return=63.0\n",
      "global_step=32532, episodic_return=91.0\n",
      "global_step=32540, episodic_return=95.0\n",
      "global_step=32548, episodic_return=95.0\n",
      "global_step=32564, episodic_return=87.0\n",
      "global_step=32572, episodic_return=95.0\n",
      "global_step=32616, episodic_return=-126.0\n",
      "global_step=32628, episodic_return=91.0\n",
      "global_step=32664, episodic_return=-375.0\n",
      "global_step=32692, episodic_return=22.0\n",
      "global_step=32700, episodic_return=95.0\n",
      "global_step=32712, episodic_return=91.0\n",
      "global_step=32740, episodic_return=75.0\n",
      "global_step=32756, episodic_return=87.0\n",
      "global_step=32760, episodic_return=-76.0\n",
      "global_step=32768, episodic_return=95.0\n",
      "Avg Training Loss: 63.40087813152172\n",
      "global_step=32776, episodic_return=-177.0\n",
      "global_step=32784, episodic_return=95.0\n",
      "global_step=32800, episodic_return=87.0\n",
      "global_step=32804, episodic_return=-71.0\n",
      "global_step=32836, episodic_return=67.0\n",
      "global_step=32900, episodic_return=-59.0\n",
      "global_step=32944, episodic_return=-130.0\n",
      "global_step=32952, episodic_return=95.0\n",
      "global_step=32968, episodic_return=23.0\n",
      "global_step=32972, episodic_return=83.0\n",
      "global_step=33004, episodic_return=60.0\n",
      "global_step=33060, episodic_return=-165.0\n",
      "global_step=33068, episodic_return=95.0\n",
      "global_step=33104, episodic_return=-370.0\n",
      "global_step=33208, episodic_return=-64.0\n",
      "global_step=33272, episodic_return=-370.0\n",
      "global_step=33304, episodic_return=-370.0\n",
      "global_step=33392, episodic_return=-7.0\n",
      "global_step=33404, episodic_return=-360.0\n",
      "global_step=33444, episodic_return=53.0\n",
      "global_step=33452, episodic_return=95.0\n",
      "global_step=33508, episodic_return=-375.0\n",
      "global_step=33572, episodic_return=-375.0\n",
      "global_step=33576, episodic_return=-45.0\n",
      "global_step=33580, episodic_return=95.0\n",
      "global_step=33596, episodic_return=87.0\n",
      "global_step=33616, episodic_return=83.0\n",
      "global_step=33624, episodic_return=95.0\n",
      "global_step=33692, episodic_return=-375.0\n",
      "global_step=33744, episodic_return=39.0\n",
      "global_step=33768, episodic_return=72.0\n",
      "Avg Training Loss: 47.75219978678615\n",
      "global_step=33808, episodic_return=-355.0\n",
      "global_step=33820, episodic_return=-133.0\n",
      "global_step=33828, episodic_return=95.0\n",
      "global_step=33876, episodic_return=-355.0\n",
      "global_step=33924, episodic_return=50.0\n",
      "global_step=34000, episodic_return=-109.0\n",
      "global_step=34068, episodic_return=-380.0\n",
      "global_step=34076, episodic_return=12.0\n",
      "global_step=34084, episodic_return=95.0\n",
      "global_step=34092, episodic_return=95.0\n",
      "global_step=34108, episodic_return=-360.0\n",
      "global_step=34224, episodic_return=-365.0\n",
      "global_step=34232, episodic_return=95.0\n",
      "global_step=34260, episodic_return=75.0\n",
      "global_step=34392, episodic_return=-380.0\n",
      "global_step=34400, episodic_return=95.0\n",
      "global_step=34404, episodic_return=-258.0\n",
      "global_step=34408, episodic_return=-370.0\n",
      "global_step=34412, episodic_return=91.0\n",
      "global_step=34416, episodic_return=95.0\n",
      "global_step=34420, episodic_return=87.0\n",
      "global_step=34424, episodic_return=95.0\n",
      "global_step=34432, episodic_return=91.0\n",
      "global_step=34536, episodic_return=-45.0\n",
      "global_step=34548, episodic_return=91.0\n",
      "global_step=34560, episodic_return=-370.0\n",
      "global_step=34564, episodic_return=87.0\n",
      "global_step=34572, episodic_return=-62.0\n",
      "global_step=34588, episodic_return=75.0\n",
      "global_step=34596, episodic_return=79.0\n",
      "global_step=34688, episodic_return=-8.0\n",
      "global_step=34696, episodic_return=95.0\n",
      "global_step=34708, episodic_return=91.0\n",
      "global_step=34716, episodic_return=95.0\n",
      "global_step=34724, episodic_return=-365.0\n",
      "global_step=34732, episodic_return=95.0\n",
      "global_step=34744, episodic_return=68.0\n",
      "global_step=34764, episodic_return=76.0\n",
      "Avg Training Loss: 47.2524264441447\n",
      "global_step=34852, episodic_return=-31.0\n",
      "global_step=34864, episodic_return=-375.0\n",
      "global_step=34888, episodic_return=-360.0\n",
      "global_step=34928, episodic_return=58.0\n",
      "global_step=34940, episodic_return=47.0\n",
      "global_step=35056, episodic_return=-121.0\n",
      "global_step=35064, episodic_return=-380.0\n",
      "global_step=35156, episodic_return=-163.0\n",
      "global_step=35200, episodic_return=-63.0\n",
      "global_step=35224, episodic_return=20.0\n",
      "global_step=35228, episodic_return=-380.0\n",
      "global_step=35344, episodic_return=-66.0\n",
      "global_step=35356, episodic_return=-355.0\n",
      "global_step=35392, episodic_return=43.0\n",
      "global_step=35432, episodic_return=56.0\n",
      "global_step=35444, episodic_return=91.0\n",
      "global_step=35524, episodic_return=-375.0\n",
      "global_step=35528, episodic_return=-365.0\n",
      "global_step=35536, episodic_return=91.0\n",
      "global_step=35580, episodic_return=44.0\n",
      "global_step=35588, episodic_return=95.0\n",
      "global_step=35624, episodic_return=-112.0\n",
      "global_step=35656, episodic_return=-365.0\n",
      "global_step=35696, episodic_return=16.0\n",
      "global_step=35792, episodic_return=-18.0\n",
      "Avg Training Loss: 48.08315252458249\n",
      "global_step=35888, episodic_return=-375.0\n",
      "global_step=35956, episodic_return=-370.0\n",
      "global_step=35984, episodic_return=70.0\n",
      "global_step=36020, episodic_return=62.0\n",
      "global_step=36080, episodic_return=33.0\n",
      "global_step=36092, episodic_return=-360.0\n",
      "global_step=36100, episodic_return=95.0\n",
      "global_step=36108, episodic_return=95.0\n",
      "global_step=36120, episodic_return=91.0\n",
      "global_step=36136, episodic_return=87.0\n",
      "global_step=36148, episodic_return=91.0\n",
      "global_step=36156, episodic_return=95.0\n",
      "global_step=36164, episodic_return=95.0\n",
      "global_step=36188, episodic_return=-370.0\n",
      "global_step=36196, episodic_return=95.0\n",
      "global_step=36216, episodic_return=46.0\n",
      "global_step=36328, episodic_return=-28.0\n",
      "global_step=36436, episodic_return=-360.0\n",
      "global_step=36488, episodic_return=-365.0\n",
      "global_step=36496, episodic_return=-370.0\n",
      "global_step=36500, episodic_return=91.0\n",
      "global_step=36508, episodic_return=95.0\n",
      "global_step=36516, episodic_return=95.0\n",
      "global_step=36620, episodic_return=-116.0\n",
      "global_step=36628, episodic_return=-385.0\n",
      "global_step=36636, episodic_return=95.0\n",
      "global_step=36648, episodic_return=91.0\n",
      "global_step=36656, episodic_return=75.0\n",
      "global_step=36668, episodic_return=83.0\n",
      "global_step=36796, episodic_return=-360.0\n",
      "global_step=36816, episodic_return=-375.0\n",
      "global_step=36824, episodic_return=-83.0\n",
      "global_step=36836, episodic_return=63.0\n",
      "global_step=36844, episodic_return=95.0\n",
      "Avg Training Loss: 22.561400935347592\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_36864' created!\n",
      "Successfully saved models!\n",
      "global_step=36884, episodic_return=30.0\n",
      "global_step=36956, episodic_return=-375.0\n",
      "global_step=37000, episodic_return=55.0\n",
      "global_step=37008, episodic_return=95.0\n",
      "global_step=37016, episodic_return=95.0\n",
      "global_step=37116, episodic_return=-219.0\n",
      "global_step=37124, episodic_return=-370.0\n",
      "global_step=37136, episodic_return=91.0\n",
      "global_step=37140, episodic_return=79.0\n",
      "global_step=37148, episodic_return=95.0\n",
      "global_step=37160, episodic_return=67.0\n",
      "global_step=37176, episodic_return=87.0\n",
      "global_step=37184, episodic_return=-360.0\n",
      "global_step=37192, episodic_return=95.0\n",
      "global_step=37196, episodic_return=83.0\n",
      "global_step=37224, episodic_return=67.0\n",
      "global_step=37236, episodic_return=91.0\n",
      "global_step=37300, episodic_return=-84.0\n",
      "global_step=37308, episodic_return=95.0\n",
      "global_step=37352, episodic_return=47.0\n",
      "global_step=37412, episodic_return=-127.0\n",
      "global_step=37420, episodic_return=95.0\n",
      "global_step=37480, episodic_return=33.0\n",
      "global_step=37484, episodic_return=-235.0\n",
      "global_step=37488, episodic_return=95.0\n",
      "global_step=37496, episodic_return=95.0\n",
      "global_step=37500, episodic_return=87.0\n",
      "global_step=37508, episodic_return=95.0\n",
      "global_step=37536, episodic_return=-355.0\n",
      "global_step=37560, episodic_return=30.0\n",
      "global_step=37652, episodic_return=-370.0\n",
      "global_step=37660, episodic_return=95.0\n",
      "global_step=37668, episodic_return=95.0\n",
      "global_step=37676, episodic_return=95.0\n",
      "global_step=37808, episodic_return=-365.0\n",
      "global_step=37824, episodic_return=87.0\n",
      "global_step=37836, episodic_return=-370.0\n",
      "global_step=37848, episodic_return=91.0\n",
      "global_step=37852, episodic_return=75.0\n",
      "global_step=37860, episodic_return=-355.0\n",
      "Avg Training Loss: 51.26626957742337\n",
      "global_step=37940, episodic_return=-218.0\n",
      "global_step=37972, episodic_return=-41.0\n",
      "global_step=38148, episodic_return=-355.0\n",
      "global_step=38160, episodic_return=-355.0\n",
      "global_step=38204, episodic_return=49.0\n",
      "global_step=38240, episodic_return=-365.0\n",
      "global_step=38272, episodic_return=-350.0\n",
      "global_step=38428, episodic_return=-83.0\n",
      "global_step=38440, episodic_return=91.0\n",
      "global_step=38448, episodic_return=-375.0\n",
      "global_step=38456, episodic_return=95.0\n",
      "global_step=38496, episodic_return=42.0\n",
      "global_step=38500, episodic_return=-268.0\n",
      "global_step=38508, episodic_return=95.0\n",
      "global_step=38540, episodic_return=-365.0\n",
      "global_step=38584, episodic_return=-6.0\n",
      "global_step=38616, episodic_return=-22.0\n",
      "global_step=38648, episodic_return=24.0\n",
      "global_step=38656, episodic_return=95.0\n",
      "global_step=38700, episodic_return=-183.0\n",
      "global_step=38840, episodic_return=-385.0\n",
      "Avg Training Loss: 30.364911280661545\n",
      "global_step=38916, episodic_return=-370.0\n",
      "global_step=38940, episodic_return=-17.0\n",
      "global_step=38956, episodic_return=-360.0\n",
      "global_step=39000, episodic_return=-375.0\n",
      "global_step=39012, episodic_return=-8.0\n",
      "global_step=39060, episodic_return=-15.0\n",
      "global_step=39160, episodic_return=-82.0\n",
      "global_step=39168, episodic_return=95.0\n",
      "global_step=39240, episodic_return=-370.0\n",
      "global_step=39312, episodic_return=-365.0\n",
      "global_step=39360, episodic_return=-365.0\n",
      "global_step=39364, episodic_return=-137.0\n",
      "global_step=39368, episodic_return=95.0\n",
      "global_step=39380, episodic_return=91.0\n",
      "global_step=39476, episodic_return=-13.0\n",
      "global_step=39504, episodic_return=-123.0\n",
      "global_step=39540, episodic_return=-375.0\n",
      "global_step=39592, episodic_return=5.0\n",
      "global_step=39632, episodic_return=59.0\n",
      "global_step=39640, episodic_return=95.0\n",
      "global_step=39656, episodic_return=-117.0\n",
      "global_step=39664, episodic_return=-365.0\n",
      "global_step=39668, episodic_return=91.0\n",
      "global_step=39672, episodic_return=95.0\n",
      "global_step=39676, episodic_return=95.0\n",
      "global_step=39772, episodic_return=-17.0\n",
      "global_step=39792, episodic_return=-23.0\n",
      "global_step=39796, episodic_return=79.0\n",
      "global_step=39840, episodic_return=-360.0\n",
      "global_step=39936, episodic_return=-238.0\n",
      "Avg Training Loss: 35.09605312353028\n",
      "global_step=39948, episodic_return=-88.0\n",
      "global_step=40016, episodic_return=8.0\n",
      "global_step=40028, episodic_return=91.0\n",
      "global_step=40052, episodic_return=72.0\n",
      "global_step=40096, episodic_return=-365.0\n",
      "global_step=40140, episodic_return=-375.0\n",
      "global_step=40172, episodic_return=-176.0\n",
      "global_step=40284, episodic_return=-120.0\n",
      "global_step=40352, episodic_return=-370.0\n",
      "global_step=40372, episodic_return=83.0\n",
      "global_step=40412, episodic_return=58.0\n",
      "global_step=40440, episodic_return=-380.0\n",
      "global_step=40456, episodic_return=52.0\n",
      "global_step=40472, episodic_return=-370.0\n",
      "global_step=40480, episodic_return=95.0\n",
      "global_step=40484, episodic_return=59.0\n",
      "global_step=40496, episodic_return=87.0\n",
      "global_step=40500, episodic_return=59.0\n",
      "global_step=40524, episodic_return=75.0\n",
      "global_step=40532, episodic_return=95.0\n",
      "global_step=40540, episodic_return=95.0\n",
      "global_step=40548, episodic_return=95.0\n",
      "global_step=40556, episodic_return=95.0\n",
      "global_step=40584, episodic_return=-355.0\n",
      "global_step=40644, episodic_return=0.0\n",
      "global_step=40652, episodic_return=95.0\n",
      "global_step=40696, episodic_return=54.0\n",
      "global_step=40736, episodic_return=-198.0\n",
      "global_step=40780, episodic_return=9.0\n",
      "global_step=40788, episodic_return=95.0\n",
      "global_step=40828, episodic_return=-181.0\n",
      "global_step=40832, episodic_return=-13.0\n",
      "global_step=40836, episodic_return=95.0\n",
      "global_step=40848, episodic_return=91.0\n",
      "Avg Training Loss: 92.00697016951835\n",
      "global_step=40980, episodic_return=-119.0\n",
      "global_step=41116, episodic_return=-220.0\n",
      "global_step=41128, episodic_return=91.0\n",
      "global_step=41132, episodic_return=-370.0\n",
      "global_step=41144, episodic_return=91.0\n",
      "global_step=41188, episodic_return=48.0\n",
      "global_step=41280, episodic_return=-360.0\n",
      "global_step=41308, episodic_return=-37.0\n",
      "global_step=41348, episodic_return=20.0\n",
      "global_step=41432, episodic_return=-370.0\n",
      "global_step=41444, episodic_return=-370.0\n",
      "global_step=41608, episodic_return=-365.0\n",
      "global_step=41644, episodic_return=-138.0\n",
      "global_step=41648, episodic_return=-370.0\n",
      "global_step=41652, episodic_return=95.0\n",
      "global_step=41656, episodic_return=-161.0\n",
      "global_step=41660, episodic_return=95.0\n",
      "global_step=41684, episodic_return=63.0\n",
      "global_step=41692, episodic_return=63.0\n",
      "global_step=41696, episodic_return=15.0\n",
      "global_step=41704, episodic_return=91.0\n",
      "global_step=41768, episodic_return=-30.0\n",
      "global_step=41800, episodic_return=-20.0\n",
      "global_step=41824, episodic_return=-47.0\n",
      "global_step=41984, episodic_return=-375.0\n",
      "Avg Training Loss: 54.351340570123284\n",
      "global_step=41992, episodic_return=95.0\n",
      "global_step=42008, episodic_return=87.0\n",
      "global_step=42068, episodic_return=-385.0\n",
      "global_step=42080, episodic_return=91.0\n",
      "global_step=42100, episodic_return=-360.0\n",
      "global_step=42104, episodic_return=79.0\n",
      "global_step=42192, episodic_return=-5.0\n",
      "global_step=42220, episodic_return=75.0\n",
      "global_step=42228, episodic_return=95.0\n",
      "global_step=42236, episodic_return=95.0\n",
      "global_step=42244, episodic_return=95.0\n",
      "global_step=42300, episodic_return=-132.0\n",
      "global_step=42308, episodic_return=-355.0\n",
      "global_step=42316, episodic_return=95.0\n",
      "global_step=42352, episodic_return=-30.0\n",
      "global_step=42404, episodic_return=0.0\n",
      "global_step=42432, episodic_return=75.0\n",
      "global_step=42448, episodic_return=87.0\n",
      "global_step=42472, episodic_return=79.0\n",
      "global_step=42584, episodic_return=-230.0\n",
      "global_step=42600, episodic_return=-380.0\n",
      "global_step=42652, episodic_return=-365.0\n",
      "global_step=42664, episodic_return=-136.0\n",
      "global_step=42672, episodic_return=95.0\n",
      "global_step=42784, episodic_return=-39.0\n",
      "global_step=42884, episodic_return=-370.0\n",
      "global_step=42900, episodic_return=-360.0\n",
      "global_step=42952, episodic_return=-350.0\n",
      "global_step=42960, episodic_return=95.0\n",
      "global_step=43000, episodic_return=-158.0\n",
      "Avg Training Loss: 24.593133392736036\n",
      "global_step=43132, episodic_return=-209.0\n",
      "global_step=43200, episodic_return=-370.0\n",
      "global_step=43240, episodic_return=-202.0\n",
      "global_step=43260, episodic_return=-370.0\n",
      "global_step=43312, episodic_return=46.0\n",
      "global_step=43320, episodic_return=-120.0\n",
      "global_step=43328, episodic_return=95.0\n",
      "global_step=43348, episodic_return=60.0\n",
      "global_step=43380, episodic_return=66.0\n",
      "global_step=43392, episodic_return=91.0\n",
      "global_step=43400, episodic_return=95.0\n",
      "global_step=43408, episodic_return=-155.0\n",
      "global_step=43480, episodic_return=10.0\n",
      "global_step=43520, episodic_return=59.0\n",
      "global_step=43528, episodic_return=95.0\n",
      "global_step=43540, episodic_return=-365.0\n",
      "global_step=43544, episodic_return=87.0\n",
      "global_step=43548, episodic_return=95.0\n",
      "global_step=43624, episodic_return=22.0\n",
      "global_step=43648, episodic_return=-375.0\n",
      "global_step=43656, episodic_return=95.0\n",
      "global_step=43700, episodic_return=5.0\n",
      "global_step=43712, episodic_return=-2.0\n",
      "global_step=43728, episodic_return=75.0\n",
      "global_step=43736, episodic_return=60.0\n",
      "global_step=43744, episodic_return=87.0\n",
      "global_step=43748, episodic_return=62.0\n",
      "global_step=43752, episodic_return=95.0\n",
      "global_step=43756, episodic_return=95.0\n",
      "global_step=43760, episodic_return=79.0\n",
      "global_step=43776, episodic_return=83.0\n",
      "global_step=43848, episodic_return=-19.0\n",
      "global_step=43944, episodic_return=-102.0\n",
      "global_step=43952, episodic_return=-244.0\n",
      "global_step=43968, episodic_return=79.0\n",
      "global_step=44000, episodic_return=-79.0\n",
      "Avg Training Loss: 63.16944021489053\n",
      "global_step=44060, episodic_return=-360.0\n",
      "global_step=44144, episodic_return=-66.0\n",
      "global_step=44152, episodic_return=95.0\n",
      "global_step=44160, episodic_return=95.0\n",
      "global_step=44168, episodic_return=95.0\n",
      "global_step=44176, episodic_return=95.0\n",
      "global_step=44204, episodic_return=-188.0\n",
      "global_step=44212, episodic_return=95.0\n",
      "global_step=44220, episodic_return=95.0\n",
      "global_step=44252, episodic_return=-370.0\n",
      "global_step=44260, episodic_return=95.0\n",
      "global_step=44264, episodic_return=-141.0\n",
      "global_step=44424, episodic_return=-82.0\n",
      "global_step=44476, episodic_return=-370.0\n",
      "global_step=44520, episodic_return=-375.0\n",
      "global_step=44528, episodic_return=95.0\n",
      "global_step=44536, episodic_return=95.0\n",
      "global_step=44540, episodic_return=-38.0\n",
      "global_step=44548, episodic_return=95.0\n",
      "global_step=44556, episodic_return=95.0\n",
      "global_step=44560, episodic_return=-380.0\n",
      "global_step=44688, episodic_return=-159.0\n",
      "global_step=44836, episodic_return=-370.0\n",
      "global_step=44844, episodic_return=-236.0\n",
      "global_step=44856, episodic_return=-365.0\n",
      "global_step=44860, episodic_return=-104.0\n",
      "global_step=44864, episodic_return=95.0\n",
      "global_step=44884, episodic_return=79.0\n",
      "global_step=44892, episodic_return=38.0\n",
      "global_step=44980, episodic_return=-3.0\n",
      "global_step=45004, episodic_return=79.0\n",
      "Avg Training Loss: 27.017513054859137\n",
      "global_step=45144, episodic_return=-355.0\n",
      "global_step=45164, episodic_return=-360.0\n",
      "global_step=45176, episodic_return=71.0\n",
      "global_step=45192, episodic_return=-385.0\n",
      "global_step=45304, episodic_return=-365.0\n",
      "global_step=45328, episodic_return=79.0\n",
      "global_step=45336, episodic_return=95.0\n",
      "global_step=45464, episodic_return=-370.0\n",
      "global_step=45476, episodic_return=-365.0\n",
      "global_step=45484, episodic_return=95.0\n",
      "global_step=45492, episodic_return=-385.0\n",
      "global_step=45508, episodic_return=87.0\n",
      "global_step=45544, episodic_return=62.0\n",
      "global_step=45636, episodic_return=-370.0\n",
      "global_step=45756, episodic_return=-223.0\n",
      "global_step=45764, episodic_return=-360.0\n",
      "global_step=45768, episodic_return=91.0\n",
      "global_step=45772, episodic_return=95.0\n",
      "global_step=45780, episodic_return=95.0\n",
      "global_step=45788, episodic_return=95.0\n",
      "global_step=45804, episodic_return=87.0\n",
      "global_step=45812, episodic_return=95.0\n",
      "global_step=45820, episodic_return=95.0\n",
      "global_step=45832, episodic_return=91.0\n",
      "global_step=45844, episodic_return=-370.0\n",
      "global_step=45936, episodic_return=-365.0\n",
      "global_step=45968, episodic_return=-142.0\n",
      "global_step=45984, episodic_return=87.0\n",
      "global_step=46000, episodic_return=-102.0\n",
      "global_step=46008, episodic_return=95.0\n",
      "global_step=46016, episodic_return=95.0\n",
      "global_step=46040, episodic_return=-28.0\n",
      "global_step=46044, episodic_return=39.0\n",
      "Avg Training Loss: 18.983098835655255\n",
      "Saving models...\n",
      "Directory './models/doom_reward_predictor/training_run_2023_07_09_15_24_08/checkpoint_step_46080' created!\n",
      "Successfully saved models!\n",
      "global_step=46144, episodic_return=-365.0\n",
      "global_step=46160, episodic_return=87.0\n",
      "global_step=46316, episodic_return=-370.0\n",
      "global_step=46340, episodic_return=-360.0\n",
      "global_step=46344, episodic_return=-370.0\n",
      "global_step=46380, episodic_return=59.0\n",
      "global_step=46408, episodic_return=-195.0\n",
      "global_step=46416, episodic_return=95.0\n",
      "global_step=46484, episodic_return=19.0\n",
      "global_step=46576, episodic_return=-3.0\n",
      "global_step=46616, episodic_return=-375.0\n",
      "global_step=46624, episodic_return=95.0\n",
      "global_step=46632, episodic_return=95.0\n",
      "global_step=46640, episodic_return=95.0\n",
      "global_step=46660, episodic_return=83.0\n",
      "global_step=46672, episodic_return=91.0\n",
      "global_step=46680, episodic_return=95.0\n",
      "global_step=46704, episodic_return=79.0\n",
      "global_step=46708, episodic_return=-365.0\n",
      "global_step=46712, episodic_return=71.0\n",
      "global_step=46716, episodic_return=95.0\n",
      "global_step=46732, episodic_return=87.0\n",
      "global_step=46800, episodic_return=-12.0\n",
      "global_step=46876, episodic_return=-365.0\n",
      "global_step=46900, episodic_return=79.0\n",
      "global_step=46984, episodic_return=-121.0\n",
      "global_step=47012, episodic_return=-380.0\n",
      "global_step=47032, episodic_return=-365.0\n",
      "Avg Training Loss: 81.4277233027203\n",
      "global_step=47200, episodic_return=-360.0\n",
      "global_step=47284, episodic_return=-365.0\n",
      "global_step=47312, episodic_return=-365.0\n",
      "global_step=47320, episodic_return=95.0\n",
      "global_step=47332, episodic_return=-370.0\n",
      "global_step=47364, episodic_return=64.0\n",
      "global_step=47368, episodic_return=50.0\n",
      "global_step=47392, episodic_return=-25.0\n",
      "global_step=47464, episodic_return=-22.0\n",
      "global_step=47472, episodic_return=95.0\n",
      "global_step=47480, episodic_return=95.0\n",
      "global_step=47500, episodic_return=-370.0\n",
      "global_step=47544, episodic_return=-69.0\n",
      "global_step=47624, episodic_return=13.0\n",
      "global_step=47644, episodic_return=-73.0\n",
      "global_step=47708, episodic_return=29.0\n",
      "global_step=47780, episodic_return=-375.0\n",
      "global_step=47800, episodic_return=-370.0\n",
      "global_step=47920, episodic_return=-32.0\n",
      "global_step=47924, episodic_return=-380.0\n",
      "global_step=48008, episodic_return=-370.0\n",
      "global_step=48080, episodic_return=-370.0\n",
      "global_step=48092, episodic_return=91.0\n",
      "global_step=48100, episodic_return=95.0\n",
      "global_step=48108, episodic_return=95.0\n",
      "global_step=48120, episodic_return=91.0\n",
      "global_step=48128, episodic_return=95.0\n",
      "Avg Training Loss: 24.986351306012693\n",
      "global_step=48136, episodic_return=95.0\n",
      "global_step=48220, episodic_return=-375.0\n",
      "global_step=48224, episodic_return=-259.0\n",
      "global_step=48228, episodic_return=-4.0\n",
      "global_step=48232, episodic_return=95.0\n",
      "global_step=48256, episodic_return=72.0\n",
      "global_step=48308, episodic_return=-370.0\n",
      "global_step=48368, episodic_return=-72.0\n",
      "global_step=48376, episodic_return=95.0\n",
      "global_step=48484, episodic_return=-175.0\n",
      "global_step=48488, episodic_return=-36.0\n",
      "global_step=48500, episodic_return=91.0\n",
      "global_step=48508, episodic_return=95.0\n",
      "global_step=48520, episodic_return=-365.0\n",
      "global_step=48524, episodic_return=87.0\n",
      "global_step=48572, episodic_return=46.0\n",
      "global_step=48608, episodic_return=-370.0\n",
      "global_step=48712, episodic_return=-72.0\n",
      "global_step=48760, episodic_return=43.0\n",
      "global_step=48784, episodic_return=-360.0\n",
      "global_step=48824, episodic_return=-380.0\n",
      "global_step=48832, episodic_return=95.0\n",
      "global_step=48844, episodic_return=91.0\n",
      "global_step=48872, episodic_return=-216.0\n",
      "global_step=48888, episodic_return=87.0\n",
      "global_step=48896, episodic_return=95.0\n",
      "global_step=48904, episodic_return=-66.0\n",
      "global_step=48920, episodic_return=87.0\n",
      "global_step=48936, episodic_return=-78.0\n",
      "global_step=49004, episodic_return=-25.0\n",
      "global_step=49008, episodic_return=-98.0\n",
      "global_step=49020, episodic_return=91.0\n",
      "Avg Training Loss: 25.731350568652715\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "observation = torch.Tensor(envs.reset()).to(agent.device)\n",
    "done = torch.zeros(num_envs).to(agent.device)\n",
    "best_avg_loss = float('+inf')\n",
    "\n",
    "for update in range(1, num_updates + 1):\n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "        global_step += num_envs\n",
    "\n",
    "        # Getting next action and it's value\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, _, value = agent.get_optimal_action_and_value(observation)\n",
    "            value = value.flatten()\n",
    "\n",
    "        observation_, reward, done_, info = envs.step(action.cpu().numpy())\n",
    "\n",
    "        # Saving experience in memory\n",
    "        memory.remember(\n",
    "            step=step, \n",
    "            observation= observation,\n",
    "            action=action,\n",
    "            value=value,\n",
    "            log_prob=log_prob,\n",
    "            reward=torch.tensor(np.array(reward, dtype=np.float32)).to(agent.device).view(-1),\n",
    "            done=done\n",
    "        )\n",
    "\n",
    "        # Saving new observation and done status for next step\n",
    "        observation = torch.Tensor(observation_).to(agent.device) \n",
    "        done =  torch.Tensor(done_).to(agent.device)\n",
    "\n",
    "        for item in info:\n",
    "            if \"episode\" in item.keys():\n",
    "                print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "                break\n",
    "    \n",
    "    # Training reward predictor\n",
    "    training_results = reward_predictor.train(memory.observations, memory.actions, memory.rewards, batch_size)\n",
    "\n",
    "    print(f\"Avg Training Loss: {training_results['avg_loss']}\")\n",
    "\n",
    "    # Logging to tensorboard\n",
    "    tensorboard_writer.add_scalar(\"charts/avg_loss\", training_results[\"avg_loss\"], global_step)\n",
    "\n",
    "    # Saving the model if current best average loss is beat\n",
    "    if training_results[\"avg_loss\"] < best_avg_loss:\n",
    "        reward_predictor.save_models(f\"./models/doom_reward_predictor/training_run_{start_datetime.strftime('%Y_%m_%d_%H_%M_%S')}/checkpoint_step_{global_step}\")\n",
    "        \n",
    "        # Saving new best average loss\n",
    "        best_avg_loss = training_results[\"avg_loss\"]\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom-rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
